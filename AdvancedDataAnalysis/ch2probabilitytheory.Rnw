

\chapter{Probability theory and probability distributions}

We begin with a review of basic probability theory.
The best book out there on probability theory that I know is the freely available book by Kerns.\cite{kerns} This chapter very closely based on this book.

As Christensen et al.\cite{christensen2011bayesian} nicely put it, for most of our data analysis goals in this course, probability is simply the area under the curve in a probability distribution function. 


\section{Kolmogorov Axioms of Probability}

[I assume some knowledge of set theory here, but I will spell this out in class.]

Let $S$ be a set of events. For example, for a single coin toss, $S=\{A_1,A_2\}$, where $A_1$ is the event that we get a  heads, and $A_2$ the event that we get a tails.

\begin{enumerate}
\item
\textbf{Axiom 1}
$(\mathbb{P}(A)\geq 0)$ for any event $(A\subset S)$.
\item
\textbf{Axiom 2}
$(\mathbb{P}(S)=1)$.
\item
\textbf{Axiom 3}
If the events $(A_{1}), (A_{2}), (A_{3})\dots$ are disjoint then


\begin{equation}
\mathbb{P}\left(\bigcup_{i=1}^{n}A_{i}\right)=\sum_{i=1}^{n}\mathbb{P}(A_{i})\mbox{ for every }n,
\end{equation}

and furthermore,

\begin{equation}
\mathbb{P}\left(\bigcup_{i=1}^{\infty}A_{i}\right)=\sum_{i=1}^{\infty}\mathbb{P}(A_{i}).
\end{equation}
\end{enumerate}

\section{Three important propositions}

%We'll be using these later on a lot.

\begin{proposition}\label{pro:p1}

Let $E\cup E^c=S$. Then,

\begin{equation}
1=P(S) = P(E \cup E^c) = P(E)+P(E^c)  
\end{equation}

or:

\begin{equation}
P(E^c) = 1-P(E)
\end{equation}
\end{proposition}

\begin{proposition}\label{pro:p2}
If $E\subset F$ then $P(E)\leq P(F)$.  
\end{proposition}

\begin{proposition}\label{pro:p3}

\begin{equation}
P(E \cup F) = P(E)+P(F)-P(EF)  
\end{equation}

%This result will be needed for a (to me) totally non-obvious outcome in multivariate distributions.
  
\end{proposition}

%\begin{proposition}\label{pro:p4}
	
%This is the inclusion-exclusion identity. See Kerns for details; we won't need it in this course.
	
%\end{proposition}

\subsection{Conditional Probability}

This is a central concept in this course.
The conditional probability of event $B$ given event $A$, denoted $\mathbb{P}(B\mid A)$, is defined by

\begin{equation}
\mathbb{P}(B\mid A)=\frac{\mathbb{P}(A\cap B)}{\mathbb{P}(A)},\quad \mbox{if }\mathbb{P}(A)>0.
\end{equation}

\begin{theorem}
For any fixed event $A$ with $\mathbb{P}(A)>0$,

\begin{enumerate}
\item $ \mathbb{P} (B|A)\geq 0 $, for all events $ B \subset S$,
\item $ \mathbb{P} (S|A) = 1 $, and
\item If $B_{1}$, $B_{2}$, $B_{3}$,... are disjoint events,
\end{enumerate}

  then:
  
  \begin{equation}
  \mathbb{P}\left(\left.\bigcup_{k=1}^{\infty}B_{k}\:\right|A\right)=\sum_{k=1}^{\infty}\mathbb{P}(B_{k}|A).
  \end{equation}
\end{theorem}

In other words, $\mathbb{P}(\cdot|A)$ is a legitimate probability function. With this fact in mind, the following properties are immediate:


For any events $A$, $B$, and $C$ with $\mathbb{P}(A)>0$,

\begin{enumerate}
\item $ \mathbb{P} ( B^{c} | A ) = 1 - \mathbb{P} (B|A).$
\item If $B\subset C$ then $\mathbb{P}(B|A)\leq\mathbb{P}(C|A)$.
\item $ \mathbb{P} [ ( B\cup C ) | A ] = \mathbb{P} (B|A) + \mathbb{P}(C|A) - \mathbb{P} [ (B \cap C|A) ].$
\item The Multiplication Rule. For any two events $A$ and $B$,

  \begin{equation}
  \mathbb{P}(A\cap B)=\mathbb{P}(A)\mathbb{P}(B|A).\label{eq-multiplication-rule-short}
  \end{equation}

  And more generally, for events $A_{1}$, $A_{2}$, $A_{3}$,..., $A_{n}$,

  \begin{equation}
  \mathbb{P}(A_{1}\cap A_{2}\cap\cdots\cap A_{n})=\mathbb{P}(A_{1})\mathbb{P}(A_{2}|A_{1})\cdots\mathbb{P}(A_{n}|A_{1}\cap A_{2}\cap\cdots\cap A_{n-1}).\label{eq-multiplication-rule-long}
  \end{equation}

\end{enumerate}

%\section{Iterated conditional probability}

%to-do: see Cameron book (prob-1.pdf in ExtraReading)

\section{Independence of events}

[Taken nearly verbatim from Kerns.]

\begin{definition}
Events A and B are said to be independent if 

\begin{equation}
\mathbb{P}(A\cap B)=\mathbb{P}(A)\mathbb{P}(B).
\end{equation}

Otherwise, the events are said to be dependent.
\end{definition}

From the above definition of conditional probability, 
we know that when $\mathbb{P}(B)>0$ we may write

\begin{equation}
\mathbb{P}(A|B)=\frac{\mathbb{P}(A\cap B)}{\mathbb{P}(B)}.
\end{equation}

In the case that A and B are independent, the numerator of the fraction factors so that $\mathbb{P}(B)$ cancels, with the result:

\begin{equation}
\mathbb{P}(A|B)=\mathbb{P}(A)\mbox{ when $A$, $B$ are independent.}
\end{equation}

\begin{proposition}
If E and F are independent events, then so are E and F$^c$, E$^c$ and F, and E$^c$ and F$^c$.

Proof:

Assume E and F are independent. Since $E=EF\cup EF^c$ and $EF$ and $EF^c$ are mutually exclusive, 

\begin{equation}
\begin{split}
P(E) =& P(EF)+P(EF^c)\\
     =& P(E)P(F)+P(EF^c)  
\end{split}	
\end{equation}

Equivalently:

\begin{equation}
\begin{split}
P(EF^c) =& P(E)[1-P(F)]\\
        =& P(E)P(F^c)
\end{split}	
\end{equation}

\end{proposition}

\section{Bayes' rule}

[Quoted nearly verbatim from Kerns.]

\begin{theorem}\label{thm:bayes}
	\textbf{Bayes' Rule}. Let $B_{1}$, $B_{2}$, ..., $B_{n}$ be mutually exclusive and exhaustive and let $A$ be an event with 
	$\mathbb{P}(A)>0$. Then 

\begin{equation}
\mathbb{P}(B_{k}|A)=\frac{\mathbb{P}(B_{k})\mathbb{P}(A|B_{k})}{\sum_{i=1}^{n}\mathbb{P}(B_{i})\mathbb{P}(A|B_{i})},\quad k=1,2,\ldots,n.\label{eq-bayes-rule}
\end{equation}	
\end{theorem}

The proof follows from looking at $\mathbb{P}(B_{k}\cap A)$ in two different ways. For simplicity, suppose that $P(B_{k})>0$ for all $k$. Then

\begin{equation}
\mathbb{P}(A)\mathbb{P}(B_{k}|A)=\mathbb{P}(B_{k}\cap A)=\mathbb{P}(B_{k})\mathbb{P}(A|B_{k}).
\end{equation}

Since $\mathbb{P}(A)>0$ we may divide through to obtain 

\begin{equation}
\mathbb{P}(B_{k}|A)=\frac{\mathbb{P}(B_{k})\mathbb{P}(A|B_{k})}{\mathbb{P}(A)}.
\end{equation}

Now remembering that  $\{B_{k}\}$ is a partition (i.e., mutually exclusive and exhaustive),  the denominator of the last expression is

\begin{equation}
\mathbb{P}(A)=\sum_{k=1}^{n}\mathbb{P}(B_{k}\cap A)=\sum_{k=1}^{n}\mathbb{P}(B_{k})\mathbb{P}(A|B_{k}).
\end{equation}


%\hfill \BlackBox

\section{Random variables}

A random variable $X$ is a function $X : S \rightarrow \mathbb{R}$ that associates to each outcome
$\omega \in S$ exactly one number $X(\omega) = x$.

$S_X$ is all the $x$'s (all the possible values of X, the support of X). I.e., $x \in S_X$. It seems we can also sloppily write $X \in S_X$ (not sure about this). 

Good example: number of coin tosses till H

\begin{itemize}
  \item $X: \omega \rightarrow x$
	\item $\omega$: H, TH, TTH,\dots (infinite)
	\item $x=0,1,2,\dots; x \in S_X$
\end{itemize}

Every discrete (continuous) random variable X has associated with it a \textbf{probability mass (distribution)  function (pmf, pdf)}. I.e., PMF is used for discrete distributions and PDF for continuous. (I will sometimes use lower case for pdf and sometimes upper case. Some books, like Christensen et al., use pdf for both discrete and continuous distributions.)

\begin{equation}
p_X : S_X \rightarrow [0, 1] 
\end{equation}

defined by

\begin{equation}
p_X(x) = P(X(\omega) = x), x \in S_X
 \end{equation}

[\textbf{Note}: Books sometimes abuse notation by overloading the meaning of $X$. They usually have: $p_X(x) = P(X = x), x \in S_X$]

Probability density functions (continuous case) or probability mass functions (discrete case) are functions that assign probabilities or relative frequencies to all events in a sample space.

The expression 

\begin{equation}
 X \sim g(\cdot)
\end{equation}

\noindent
means that the random variable $X$ has pdf/pmf $g(\cdot)$.
For example, if we say that $X\sim N(\mu,\sigma^2)$, we are assuming that the pdf is

\begin{equation}
f(x)= \frac{1}{\sqrt{2\pi \sigma^2}} \exp[-\frac{(x-\mu)^2}{2\sigma^2}]
\end{equation}

We also need a \textbf{cumulative distribution function} or cdf because, in the continuous case, P(X=some point value) is zero and we therefore need a way to talk about P(X in a specific range). cdfs serve that purpose.

In the continuous case, the cdf or distribution function is defined as: 

\begin{equation}
P(x<X) = F(x<X) =\int_{-\infty}^{X} f(x)\, dx
\end{equation}

\textbf{Note}: Almost any function can be a pdf as long as it sums to 1 over the sample space. Here is an example of a function that doesn't sum to 1:

\begin{equation}
f(x)=\exp[-\frac{(x-\mu)^2}{2 \sigma^2}]
\end{equation}

This is (I think this is what it's called) the ``kernel'' of the normal pdf, and it doesn't sum to 1:

<<>>=
normkernel<-function(x,mu=0,sigma=1){
  exp((-(x-mu)^2/(2*(sigma^2))))
}

x<-seq(-10,10,by=0.01)

plot(function(x) normkernel(x), -3, 3,
      main = "Normal density",ylim=c(0,1),
              ylab="density",xlab="X")

## area under the curve:
integrate(normkernel,lower=-Inf,upper=Inf)
@

Adding a normalizing constant makes the above kernel density a pdf.

<<>>=
norm<-function(x,mu=0,sigma=1){
  (1/sqrt(2*pi*(sigma^2))) * exp((-(x-mu)^2/(2*(sigma^2))))
}

x<-seq(-10,10,by=0.01)

plot(function(x) norm(x), -3, 3,
      main = "Normal density",ylim=c(0,1),
              ylab="density",xlab="X")

## area under the curve:
integrate(norm,lower=-Inf,upper=Inf)
@


%[\textbf{to-do: add proof that area under normal sums to 1.}]

Recall that 
a random variable $X$ is a function $X : S \rightarrow \mathbb{R}$ that associates to each outcome
$\omega \in S$ exactly one number $X(\omega) = x$.
$S_X$ is all the $x$'s (all the possible values of X, the support of X). I.e., $x \in S_X$.

$X$ is a continuous random variable if there is a non-negative function $f$ defined for all real $x \in (-\infty,\infty)$ having the property that for any set B of real numbers, 

%(note that B is the support $S_X$ in Kerns' notation; the use of B is Ross' notation),

\begin{equation}
P\{X \in B\} = \int_B f(x) \, dx 
\end{equation}

Kerns has the following to add about the above:

\begin{quote}
Continuous random variables have supports that look like
  
	\begin{equation}
	S_{X}=[a,b]\mbox{ or }(a,b),
	\end{equation}
	
	or unions of intervals of the above form. Examples of random variables that are often taken to be continuous are:

\begin{itemize}
\item the height or weight of an individual,
\item other physical measurements such as the length or size of an object, and
\item durations of time (usually).
\end{itemize}

E.g., in linguistics we take as continous: 

\begin{enumerate}
\item reading time: Here the random variable X has possible values $\omega$ ranging from 0 ms to 
some upper bound b ms, and the RV X maps each possible value $\omega$ to the corresponding number (0 to 0 ms, 1 to 1 ms, etc.). 
\item acceptability ratings  (technically not true; but people generally treat ratings as continuous, at least in psycholinguistics)
\item EEG signals
\end{enumerate}

Every continuous random variable $X$ has a probability density function (PDF) denoted $f_{X}$ associated with it
	that satisfies three basic properties:

\begin{enumerate}
\item $f_{X}(x)>0$ for $x\in S_{X}$,
\item $\int_{x\in S_{X}}f_{X}(x)\,\mathrm{d} x=1$, and
\item  $\mathbb{P}(X\in A)=\int_{x\in A}f_{X}(x)\:\mathrm{d} x$, for an event $A\subset S_{X}$.
\end{enumerate}

	We can say the following about continuous random variables:

\begin{itemize}
\item Usually, the set $A$ in condition 3 above takes the form of an interval, for example, $A=[c,d]$, in which case

	  \begin{equation}
	  \mathbb{P}(X\in A)=\int_{c}^{d}f_{X}(x)\:\mathrm{d} x.
	  \end{equation}

\item It follows that the probability that $X$ falls in a given interval is simply the area under the curve of $f_{X}$ over the interval.
\item Since the area of a line $x=c$ in the plane is zero, $\mathbb{P}(X=c)=0$  for any value $c$. In other words, the chance that $X$ equals a particular value $c$ is zero, and this is true for any number $c$. Moreover, when $a<b$ all of the following probabilities are the same:

	  \begin{equation}
	  \mathbb{P}(a\leq X\leq b)=\mathbb{P}(a<X\leq b)=\mathbb{P}(a\leq X<b)=\mathbb{P}(a<X<b).
	  \end{equation}
\item The PDF $f_{X}$ can sometimes be greater than 1. This is in contrast to the discrete case; every nonzero value of a PMF is a probability which is restricted to lie in the interval $[0,1]$.
\end{itemize}
\end{quote}

$f(x)$ is the probability density function of the random variable $X$.

Since $X$ must assume some value, $f$ must satisfy

\begin{equation}
1= P\{X \in (-\infty,\infty)\} = \int_{-\infty}^{\infty} f(x) \, dx 
\end{equation}

If $B=[a,b]$, then 

\begin{equation}
P\{a \leq X \leq b\} = \int_{a}^{b} f(x) \, dx 
\end{equation}

If $a=b$, we get

\begin{equation}
P\{X=a\} = \int_{a}^{a} f(x) \, dx = 0
\end{equation}

Hence, for any continuous random variable, 

\begin{equation}
P\{X < a\} = P \{X \leq a \} = F(a) = \int_{-\infty}^{a} f(x) \, dx 
\end{equation}

$F$ is the \textbf{cumulative distribution function}. Differentiating both sides in the above equation:

\begin{equation}
\frac{d F(a)}{da} = f(a) 
\end{equation}

The density (PDF) is the derivative of the CDF. 

\begin{center}
\begin{fmpage}{\linewidth}
\textbf{Basic results (proofs omitted)}:

\begin{enumerate}
	\item \begin{equation}
	E[X]= \int_{-\infty}^{\infty} x f(x) \, dx
	\end{equation}
\item
	\begin{equation}
	E[g(X)]= \int_{-\infty}^{\infty} g(x) f(x) \, dx
	\end{equation}
\item
	\begin{equation}
	E[aX+b]= aE[X]+b
	\end{equation}
\item
	\begin{equation}
	Var[X]= E[(X-\mu)^2]=E[X^2]-(E[X])^2
	\end{equation}
\item
	\begin{equation}
	Var(aX+b)= a^2Var(X)
	\end{equation}	
\end{enumerate}

\end{fmpage}
\end{center}

So, so far, we know what a random variable is, and we know that by definition it has a pdf and a cdf associated with it. 

\section{What can you do with a pdf?}

You can:

\begin{enumerate}
\item
Calculate the mean:

Discrete case:

\begin{equation}
E[X]= \underset{i=1}{\overset{n}{\sum}} x_i p(x_i)
\end{equation}

Continuous case:

\begin{equation}
E[X]= \int_{-\infty}^{\infty} x f(x) \, dx
\end{equation}

\item 
Calculate the variance:

\begin{equation}
  Var(X)= E[X^2] - (E[X])^2
\end{equation}
\item 
Compute quartiles: e.g., for some pdf f(x):

\begin{equation}
\int_{-\infty}^{Q} f(x)\, dx
\end{equation}

For example, take $f(x)$ to be the normal distribution with mean 0 and sd 1. Suppose we want to know:

\begin{equation}
\int_{0}^{1} f(x)\, dx
\end{equation}

We can do this in R as follows:\footnote{This is a very important piece of R code here. Make sure you understand the relationship between the integral and the R functions used here.} 

<<>>=
integrate(function(x) dnorm(x, mean = 0, sd = 1),
lower=0,upper=1)
## alternatively:
pnorm(1)-pnorm(0)
@
\end{enumerate}


\begin{Homework}\label{hweasyintegral}

\textbf{This assignment is optional.}

Suppose we are given that the pdf of $\theta$, which ranges from 0 to 1, is proportional to 
$\theta^2$. This means that there is some constant c (the constant of proportionality) such that $1=c \int_0^1 \theta^2\, d\theta$.

\begin{enumerate}
\item Find c.
\item Find the mean, median (hint: what is the median in terms of quantiles?) and variance of the above pdf.
\item Find the 95\% credible interval, i.e., the lower and upper values in $P(lower < \theta < upper)= 0.95$.
\end{enumerate}
\end{Homework}
%%source is Christensen I think.

\section{Some basic facts about expectation and variance}

\begin{enumerate}
\item Computing expectation:

\begin{equation}
  E[X]= \underset{i=1}{\overset{n}{\sum}} x_i p(x_i)
\end{equation}

\begin{equation}
  E[X]= \int_{-\infty}^{\infty} x f(x) \, dx
	\end{equation}
  
\item Computing expectation of a function of a random variable:

\begin{equation}
	E[g(X)]= \underset{i=1}{\overset{n}{\sum}} g(x_i) p(x_i)
\end{equation}

  \begin{equation}
	E[g(X)]= \int_{-\infty}^{\infty} g(x) f(x) \, dx
	\end{equation}
\item Computing variance:
\begin{equation}
	Var(X)= E[(X-\mu)^2]
\end{equation}

\begin{equation}
	Var(X)= E[X^2] - (E[X])^2
\end{equation}
\item Computing variance of a linear transformation of an RV:

\begin{equation}
	Var(aX+b)= a^2 Var(X)
\end{equation}

[Notice that later on in matrix form we will get:   $Var(AX+b)= A Var(X) A'$ for linear transformations like $y=AX + b$.]


\item
\begin{equation}
	SD(X)=\sqrt{Var(X)}
\end{equation}

\item
For two independent random variables $X$ and $Y$, 

\begin{equation}
E[XY]=E[X]E[Y]
\end{equation}

\item
Covariance of two random variables:

\begin{equation}
Cov(X,Y)=E[(X-E[X]) (Y - E[Y])]
\end{equation}

\item
Note that Cov(X,Y)=0 if X and Y are independent.

Corollary in 4.1 of Ross:\cite{RossProb}

\begin{equation}
E[aX + b] = aE[X]+b
\end{equation}

\item
A related result is about \textbf{linear combinations of RVs}:

\begin{theorem}
Given two \textbf{not necessarily independent} random variables X and Y:


\begin{equation}
E[aX + bY] =aE[X] + bE[Y]
\end{equation}
\end{theorem}

\item
If X and Y are independent, 

\begin{equation}
Var(X+Y)=Var[X] + Var[Y]
\end{equation}

and

\begin{equation}
Var(aX+bY)=a^2Var(X) + b^2Var(Y)
\end{equation}

If $a=1, b=-1$, then

\begin{equation}
Var(X-Y)=Var(X) + Var(Y)
\end{equation}

\item
If X and Y are not independent, then

\begin{equation}
Var(X-Y)=Var(X) + Var(Y) -2 Cov (X,Y)
\end{equation}

\end{enumerate}

\section{Three common (and, for us, important) distributions}\label{commondistrn}


\paragraph{Binomial distribution}

If we have $x$ successes in $n$ trials, given a success probability $p$ for each trial. If $x \sim Bin(n,p)$.

\begin{equation}
P(x\mid n, p) = {n \choose k} p^k (1-p)^{n-k} 
\end{equation}

[Recall that: ${n \choose k} = \frac{n!}{(n-r)! r!}$]

The mean is $np$ and the variance $np(1-p)$.

When $n=1$ we have the Bernoulli distribution.

\begin{verbatim}
##pmf:
dbinom(x, size, prob, log = FALSE)
## cdf:
pbinom(q, size, prob, lower.tail = TRUE, log.p = FALSE)
## quantiles:
qbinom(p, size, prob, lower.tail = TRUE, log.p = FALSE)
## pseudo-random generation of samples:
rbinom(n, size, prob)
\end{verbatim}

\paragraph{Uniform random variable}

A random variable $(X)$ with the continuous uniform distribution on the interval $(\alpha,\beta)$ has PDF

\begin{equation}
f_{X}(x)=
\begin{cases}
\frac{1}{\beta-\alpha}, & \alpha < x < \beta,\\
0 , & \hbox{otherwise}
\end{cases}
\end{equation}

The associated $\mathsf{R}$ function is $\mathsf{dunif}(\mathtt{min}=a,\,\mathtt{max}=b)$. We write $X\sim\mathsf{unif}(\mathtt{min}=a,\,\mathtt{max}=b)$. Due to the particularly simple form of this PDF we can also write down explicitly a formula for the CDF $F_{X}$:

\begin{equation}
F_{X}(a)=
\begin{cases}
0, & a < 0,\\
\frac{a-\alpha}{\beta-\alpha}, & \alpha \leq t < \beta,\\
1, & a \geq \beta.
\end{cases}
\label{eq-unif-cdf}
\end{equation}

\begin{equation}
E[X]= \frac{\beta+\alpha}{2}
\end{equation}

\begin{equation}
Var(X)= \frac{(\beta-\alpha)^2}{12}
\end{equation}

\begin{verbatim}
dunif(x, min = 0, max = 1, log = FALSE)
punif(q, min = 0, max = 1, lower.tail = TRUE, log.p = FALSE)
qunif(p, min = 0, max = 1, lower.tail = TRUE, log.p = FALSE)
runif(n, min = 0, max = 1)
\end{verbatim}

\paragraph{Normal random variable}

\begin{equation}
f_{X}(x)=\frac{1}{\sigma\sqrt{2\pi}}e^{ \frac{-(x-\mu)^{2}}{2\sigma^{2}}},\quad -\infty < x < \infty.
\end{equation}

We write $X\sim\mathsf{norm}(\mathtt{mean}=\mu,\,\mathtt{sd}=\sigma)$, and the associated $\mathsf{R}$ function is \texttt{dnorm(x, mean = 0, sd = 1)}.

\begin{marginfigure} 
<<fig=TRUE,echo=FALSE,width=3,height=3>>=
plot(function(x) dnorm(x), -3, 3,
      main = "Normal density",ylim=c(0,.4),
              ylab="density",xlab="X")
@
\caption{Normal distribution.} \label{fig:normaldistr}
\end{marginfigure}

If $X$ is normally distributed with parameters $\mu$ and $\sigma^2$, then $Y=aX+b$ is normally distributed with parameters $a\mu + b$ and $a^2\sigma^2$.

\textbf{Standard or unit normal random variable:} 

If $X$ is normally distributed with parameters $\mu$ and $\sigma^2$, then $Z=(X-\mu)/\sigma$ is normally distributed with parameters $0,1$.

We conventionally write $\Phi (x)$ for the CDF:

\begin{equation}
\Phi (x)=\frac{1}{\sqrt{2\pi}} \int_{-\infty}^{x}  e^{\frac{-y^2}{2}} \, dy 
\quad \textrm{where}~y=(x-\mu)/\sigma
\end{equation}

Old-style (pre-computer era) printed tables give the values for positive $x$; for negative $x$ we do:

\begin{equation}
\Phi (-x)= 1- \Phi (x),\quad -\infty < x < \infty
\end{equation}

If $Z$ is a standard normal random variable (SNRV) then

\begin{equation}
p\{ Z\leq -x\} = P\{Z>x\}, \quad -\infty < x < \infty
\end{equation}

Since $Z=((X-\mu)/\sigma)$ is an SNRV whenever $X$ is normally distributed with parameters $\mu$ and $\sigma^2$, then the CDF of $X$ can be expressed as:

\begin{equation}
F_X(a) = P\{ X\leq a \} = P\left( \frac{X - \mu}{\sigma} \leq \frac{a - \mu}{\sigma}\right) = \Phi\left( \frac{a - \mu}{\sigma} \right)
\end{equation}

The standardized version of a normal
random variable X is used to compute specific probabilities relating to X (it's also easier to compute probabilities from different CDFs so that the two computations are comparable).

\begin{verbatim}
dnorm(x, mean = 0, sd = 1, log = FALSE)
pnorm(q, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE)
qnorm(p, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE)
rnorm(n, mean = 0, sd = 1)
\end{verbatim}

\textbf{The expectation of the standard normal random variable}:

Here is how we can calculate the expectation of an SNRV.

\begin{equation*}
E[Z] = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^\infty x e^{-x^2/2} \, dx
\end{equation*}

Let $u = -x^2/2$.

Then, $du/dx = -2x/2=-x$. I.e., $du= -x \, dx$ or $-du=x \, dx$.

We can rewrite the integral as:

\begin{equation*}
E[Z]  = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^\infty e^{u} x \, dx
\end{equation*}

Replacing $x\, dx$ with $-du$ we get:

\begin{equation*}
-\frac{1}{\sqrt{2\pi}} \int_{-\infty}^\infty e^{u} \, du  
\end{equation*}

which yields:

\begin{equation*}
-\frac{1}{\sqrt{2\pi}} [ e^{u} ]_{-\infty}^{\infty}
\end{equation*}

Replacing $u$ with $-x^2/2$ we get:

\begin{equation*}
-\frac{1}{\sqrt{2\pi}} [ e^{-x^2/2} ]_{-\infty}^{\infty} = 0
\end{equation*}
 
\textbf{The variance of the standard normal distribution}:

We know that 

\begin{equation*}
\hbox{Var}(Z)=E[Z^2]-(E[Z])^2
\end{equation*}

Since $(E[Z])^2=0$ (see immediately above), we have

\begin{equation*}
\hbox{Var}(Z)=E[Z^2] = 
\frac{1}{\sqrt{2\pi}} \int_{-\infty}^\infty \explain{x^2}{\textrm{This is $Z^2$.}}  e^{-x^2/2}  \, dx
\end{equation*}

Write $x^2$ as $x\times x$ and use integration by parts:

\begin{equation*}
\frac{1}{\sqrt{2\pi}} \int_{-\infty}^\infty 
\explain{x}{u} \explain{x e^{-x^2/2}}{dv/dx} \, dx =
\frac{1}{\sqrt{2\pi}}\explain{x}{u} \explain{-e^{-x^2/2}}{v} -
\frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty \explain{-e^{-x^2/2}}{v} 
\explain{1}{du/dx} \, dx = 1
\end{equation*} 

[Explained on p.\ 274 of Grinstead and Snell's online book\cite{GrinsteadSnell}; it wasn't obvious to me, and Ross\cite{RossProb} is pretty terse]:
``The first summand above can be shown to equal 0, since as 
$x \rightarrow \pm \infty$, 
$e^{−x^2/2}$
gets
small more quickly than $x$ gets large. The second summand is just the standard
normal density integrated over its domain, so the value of this summand is 1.
Therefore, the variance of the standard normal density equals 1.''

\begin{center}
\begin{fmpage}{\linewidth}
\textbf{Example}:	
Given $X\sim N(10,16)$, write distribution of $\bar{X}$, where $n=4$. Since $SE=sd/sqrt(n)$, the distribution of $\bar{X}$ is $N(10,16/4)$.
\end{fmpage}
\end{center}


\section{Distribution of a function of a random variable (transformations of random variables)}

[This section can be skipped.]

A nice and intuitive description:

Consider a continuous RV Y which is a continuous differentiable increasing function of X:

\begin{equation}
Y=g(X)	
\end{equation}

Because g is differentiable and increasing, $g'$ and $g^{-1}$ are guaranteed to exist. Because g maps all $x\leq s \leq x+\Delta x$ to 
$y\leq s \leq y+\Delta y$, we can say:

\begin{equation}
\int_x^{x+\Delta x} f_X(s)\, ds = \int_y^{y+\Delta y} f_Y(t)\, dt	
\end{equation}

Therefore, for small $\Delta x$:

\begin{equation}
f_Y(y)\Delta y \approx f_X(x)\Delta x	
\end{equation}

Dividing by $\Delta y$ we get:

\begin{equation}
f_Y(y) \approx f_X(x)\frac{\Delta x}{\Delta y}
\end{equation}



\begin{theorem}
[\textbf{Theorem 7.1 in Ross}]
Let $X$ be a continuous random variable having probability density function $f_X$.
Suppose that $g(x)$ is a strict monotone (increasing or decreasing) function, differentiable and (thus continuous) function of $x$. Then the random variable $Y$ defined by $Y=g(X)$ has a probability density function defined by 

\begin{equation*}
f_Y(y)=  \left\{ 	
\begin{array}{l l}
       f_X(g^{-1}(y)) \mid \frac{d}{dx} g^{-1}(y) \mid  & \quad \textrm{if } y = g(x) \textrm{ for some $x$}\\
       0 & \quad \textrm{if } y\neq g(x) \textrm{ for all $x$}.\\
\end{array} \right.
\end{equation*}

\noindent
where $g^{-1}(y)$ is defined to be equal to the value of $x$ such that $g(y-y)$.

%[to-do: Ross writes $\frac{d}{dx} g^{-1}(y)$ as an absolute %$\mid \frac{d}{dx} g^{-1}(y) \mid$. Need to understand why.]

\end{theorem}

Proof:

Suppose $y=g(x)$ for some $x$. Then, with $Y=g(X)$,

\begin{equation}
\begin{split}
F_Y(y) =& P(g(X)\leq y)\\
	=& P(X\leq g^{-1}(y))\\
	=& F_X(g^{-1}(y))\\
\end{split}	
\end{equation}

%\begin{equation}\label{theorem7.1a}
%F_Y(y) = F_X(g^{-1}(y))
%\end{equation}

Differentiation gives

\begin{equation} \label{theorem7.1b}
	f_Y(y) = f_X(g^{-1}(y)) \frac{d(g^{-1}(y))}{dy}	
\end{equation}

Detailed explanation for the above equation:
Since

\begin{equation} \label{theorem7.1c}
\begin{split}	
F_Y(y) =& F_X(g^{-1}(y)) \\
       =& \int f_X(g^{-1}(y)) \, dy\\
\end{split}
\end{equation}

Differentiating: 

\begin{equation} \label{theorem7.1d}
\begin{split}	
\frac{d(F_Y(y))}{dy}=& \frac{d}{dy}(F_X(g^{-1}(y)))
\end{split}
\end{equation}

We use the chain rule. To simplify things, rewrite $w(y)=g^{-1}(y)$ (otherwise typesetting things gets harder). Then, let

\begin{equation*}
u = w(y)	
\end{equation*}

which gives

\begin{equation*}
\frac{du}{dy} = w'(y)	
\end{equation*}

and let

\begin{equation*}
x = F_X(u)	
\end{equation*}

This gives us 

\begin{equation*}
\frac{dx}{du} = F'_X(u) = f_X(u)
\end{equation*}

By the chain rule:

\begin{equation*}
\frac{du}{dy} \times \frac{dx}{du} = w'(y) f_X(u) \explain{=}{\textrm{plugging in the variables}} 
\frac{d}{dy}(g^{-1}(y)) f_X(g^{-1}(y))  
\end{equation*}

\hfill \BlackBox

\begin{center}
\begin{fmpage}{0.9\linewidth}
\textbf{Exercises (optional)}:	
\begin{enumerate}
	\item $Y=X^2$
	\item $Y=\sqrt X$
	\item $Y=\mid X \mid$
	\item $Y = aX + b$ %(see document gst2.pdf)
\end{enumerate}
\end{fmpage}
\end{center}


\section{Class activities}

\subsection{Exercise 1: Actual coin toss experiment}

Toss a coin 10 times and compute, using \texttt{pbinom}, the probability of getting the total numbers of heads you got, assuming that the coin is fair.

Hint: given sample size $n$, your assumed probability of a heads $prob$, and the number of heads you got $x$, the \texttt{pbinom} function delivers $P(X \leq x)$, the probability of getting $x$ heads or less. In other words, pbinom is the \textbf{cumulative distribution function} (textbooks often call this simply \textbf{distribution function}).

Here is how you can compute $P(X\leq x)$:

\begin{verbatim}
pbinom(x,size=n,p=prob)
\end{verbatim}

<<label=solex1,echo=F,eval=F>>=
nheads<-8
pbinom(nheads,size=10,p=0.5)-pbinom((nheads-1),size=10,p=0.5)
@

Note that you have to compute $P(X=x)$!

Based on what everyone finds, we can write down the number of cases we have of each possible outcome, along with the theoretical probability. 

\begin{table}[ht]
\centering
\begin{tabular}{rlllllllllll}
  \hline
Number of heads: & 0 & 1 & 2 & 3 & 4 & 5 &  6 & 7 & 8 & 9 & 10 \\ 
  \hline
Theoretical probability: &  &  &  &  &  &  &  &  &  & &  \\ 
Count: &  &  &  &  &  &  &  &  &  &  \\ 
   \hline
\end{tabular}
\end{table}

<<echo=F,eval=F>>=
nstud<-23
store<-rep(NA,nstud)
headcount<-rep(NA,nstud)
for(i in 1:nstud){
nheads<-rbinom(n=1,size=10,p=0.5)
headcount[i]<-nheads
store[i]<-pbinom(nheads,size=10,p=0.5)-
          pbinom(nheads-1,size=10,p=0.5)
}

pbinom(5,size=10,p=0.5)-pbinom(4,size=10,p=0.5)


table(headcount)/23

@


\subsection{Exercise 2: Probability theory}

Given 
$X \sim f(\cdot)$, where $f(\cdot)$ is (a) $Unif(0,10)$, (b) 
$N(\mu=100,\sigma^2=20)$, (c) 
$Binom(p=.6,n=20)$. 
Find the probability of $P(X<3), P(X>11), P(X=6)$ 
for each distribution.

Fill in the table below.


\begin{table}[ht]
\centering
\begin{tabular}{rrr}
  \hline
$f(\cdot)$ & Prob.\ & Answer  \\ 
  \hline
Unif(0,10) & $P(X<3)=$ &  \\ 
   & $ P(X>11)=$ &    \\ 
   & $P(X=6)=$  &  \\ 
   
 N(100,20) & $P(X<3)=$ &  \\ 
   & $ P(X>11)=$  &    \\ 
   & $P(X=6)=$  &  \\ 
 Binom(p=.6,n=20) & $P(X<3)=$ &  \\ 
   & $ P(X>11)=$  &    \\ 
   & $P(X=6)=$  &  \\ 
   \hline
\end{tabular}
\end{table}


<<label=solex2,echo=F,eval=F>>=
punif(11,min=0,max=10,lower.tail=F)

punif(6,min=0,max=10)-punif(5,min=0,max=10)
@


\subsection{Exercise 3: Normally distributed data with known variance}

Here we will consider normally distributed data with unknown mean but known variance.

Before getting into the exercise, I want to first introduce the idea of the \textbf{likelihood function}.

Suppose $X_1,\dots,X_n$ is a random variable that comes from a normally distributed population with mean $\mu$ and variance $\sigma^2$. 
Suppose that the sample values $x_1,\dots,x_n$ are independent.

Because these values are independent, 
the joint probability of getting these values is just the product of the individual probabilities:

\begin{equation}
\begin{split}
P(X_1=x_1,X_2=x_2,\dots,X_n=x_n) =& P(X_1=x_1)P(X_2=x_2)\dots P(X_n=x_n) \\
=& f(X_1=x_1,X_2=x_2,\dots,X_n=x_n;\theta)  \\
\end{split}
\end{equation}

The last line $f(X_1=x_1,X_2=x_2,\dots,X_n=x_n;\theta)$ means: ``the joint probability of the values $x_1,\dots,x_n$ given a specific value for the parameter(s) $\theta$.'' Here, $\theta$ is the vector $<\mu>$.\footnote{If $\sigma^2$ were unknown, $\theta$ would be $<\mu,\sigma^2>$.} 

For different values of $\theta$, we would get different values for the function $f(\cdot)$. In other words, we can talk about a function  $L(\theta)$ given the data, that delivers different values for each value of $\theta$. 
This is called the \textbf{likelihood function}.

In other words:

\begin{eqnarray}
L(x\mid \theta) & = \prod N(x_i; \mu, \sigma^2)  \\
                 & = (\frac{1}{\sigma \sqrt{2 \pi}})^{n} \exp (-\frac{1}{2\sigma^2} \sum (x_i - \mu)^2)\\ 
\end{eqnarray}

We assume that we already know $\sigma^2$. Given the likelihood function, we can ask:
What is the value of $\mu$ that would maximize the probability of this data?

Taking logs and differentiating with respect to $\mu$, we get:

\begin{equation}
	\hat \mu = \frac{1}{n}\sum x_i = \bar{x}	
\end{equation}

%and

%\begin{equation}
%	\hat \sigma ^2 = \frac{1}{n}\sum (x_i-\bar{x})^2
%\end{equation}

%That's what we do when we analyze data: we take the \textbf{maximum %likelihood estimate} of $\mu$, which is $\bar{x}$. Note that when we %compute sample variance, we use a different formula than the maximum %likelihood estimate of $\sigma^2$.\marginnote{We use:

%\begin{equation}
%\hat \sigma ^2 = \frac{1}{n-1}\sum (x_i-\bar{x})^2
%\end{equation}

%This is because the MLE for $\sigma^2$ delivers a biased estimate, and %this correction procedure corrects for the bias. Note that as $n$ %approaches infinity, dividing by $n$ or $n-1$ is not going to matter.
%} 

Suppose that we have independent data as follows:

<<>>=
x<-c(15.7,14.7,16.1,16.2,15.1)
@

We typically start by calculating the mean (among other things):

<<>>=
mean(x)
@

Using this sample mean (and the given variance), we make a best guess as to the pdf that is assumed to have generated \textbf{each data point}:

\bigskip
$X\sim N(\bar{x},\sigma^2)$
\bigskip

For $n$ data points (as you will no doubt recall from the introductory course in the summer semester), we can characterize the sampling distribution of the sample means as:

\bigskip
$\bar{X}\sim N(\bar{x},\sigma^2/n)$
\bigskip

%More generally, the method of maximum likelihood consists of maximizing %the likelihood function with respect to $\theta$. The value of $\theta$ %that maximizes the likelihood function is the \textbf{MLE} (maximum %likelihood estimate) of $\theta$.

So let us suppose we have independent and identically distributed data $x=\{x_1,\dots,x_n\}$, where $X_i \sim N(\mu,\sigma^2)$. Assume that $\sigma^2$ is known as above, but $\mu$ is unknown.  

It follows that the pdf for random variable representing any one data point is $N(\bar{x},\sigma^2)$. 

Now we make the big bayesian move. Let our prior be that the sampling distribution of $\mu$ is  $N(m,v)$. Note that this is the prior for the sampling distribution of the mean $\mu$. It represents what we believe to be true about $\mu$, including our degree of uncertainty about our belief. 

Then, given Bayes' theorem

\bigskip

$\hbox{Posterior} \propto \hbox{Prior} \times \hbox{Likelihood}$

\bigskip

or

\bigskip

$\hbox{Posterior} \propto \explain{N(m,v)}{prior} \times \explain{N(\bar{x},\sigma^2/n)}{likelihood}$

\bigskip

it is straightforward to derive analytically (proof omitted, but see Lynch textbook) that the posterior will be $N(m*,v*)$, where

\begin{equation}
v*=\frac{1}{\frac{1}{v}+ \frac{n}{\sigma^2}} 
\end{equation}

\begin{equation}
m*= v* \left( \frac{m}{v} + \frac{n\bar{x}}{\sigma^2} \right)
\end{equation}

Importantly, 
we can rewrite m* as a weighted mean of the prior and the sample mean:

\begin{equation}
m* = \frac{w_1}{w_1+w_2} m + \frac{w_2}{w_1+w_2} \bar{x}
\end{equation}

where $w_1=v^{{-1}}$ and $w_2 = (\frac{\sigma^2}{n})^{-1}$ 

Because of results like the one above, it is easier to talk about variance in terms of \textbf{precision=1/variance}. In JAGS and pretty much any other bayesian data analysis software, everything is stated in terms of precision, e.g., normal distributions are defined as N(mean,precision), not N(mean,variance).\marginnote[-2cm]{Note the differences between R and JAGS/BUGS, and statistical conventions in textbooks:

\begin{enumerate}
\item In textbooks, the normal distribution is represented by $N(\mu,\sigma^2)$, i.e., the uncertainty is in terms of \textbf{variance}.
\item
In R, we have  $N(\mu,\sigma)$, i.e., the uncertainty is in terms of \textbf{standard deviation}.
\item In JAGS and BUGS in general, we have $N(\mu,\frac{1}{\sigma^2})$, i.e., the uncertainty is in terms of \textbf{precision}.
\end{enumerate}

I'm sorry for this confusing mess, but it's the statisticans' fault that they don't keep things straight. \textbf{Don't forget this detail in the coming lectures.}
}

\textbf{Example of how we can use the above analytical result}:

Let the data be: 15.7,14.7,16.1,16.2,15.1. Variance ($\sigma^2$) known at 0.6.
Let the prior be  N(m=14,v=1).
We will work out the posterior distribution using the formula and using JAGS. 

<<>>=
derive.post<-function(n,x.bar,sigma2,m,v){
v.star<- 1/( (1/v) + n/sigma2 )
m.star<-v.star*(m/v + (n*x.bar/sigma2))
return(list(m.star,v.star))
}

data<-list(y=c(15.7,14.7,16.1,16.2,15.1))
## let this data come from N(mu,sigma2=.6)
## Let prior be N(14,1)

(post<-derive.post(n=length(data$y),
                   x.bar=mean(data$y),
                   sigma2=0.6,m=14,v=1))
@

You can now play with the weighting of the prior and data.

First, make the prior variance very low; this yields a shift of the posterior towards the prior, with low variance:

<<>>=
(post<-derive.post(n=length(data$y),x.bar=mean(data$y),
                   sigma2=.6,m=14,v=0.0001))
@

Next, make the prior variance very high; this shifts the posterior towards the sample mean and variance:

<<>>=
(post<-derive.post(n=length(data$y),x.bar=mean(data$y),
                   sigma2=.6,m=14,v=100))
mean(data$y); var(data$y)
@

This basically sums up a situation we will encounter in this course: we will use priors that express low certainty (``non-informative'' priors), letting ``the data speak for themselves.'' But in cases where we do have prior information, we will use it! (This is impossible to do in frequentist settings) You will see how prior beliefs can be incorporated into your analysis.

\textbf{Question}: what role does sample size n play in the computation of the posterior? When sample size is increased in the above example, 
will the posterior lean towards the prior or the likelihood?


Next, we fit a JAGS model to do the same calculation that we did analytically above, but now
using something called \textbf{Monte Carlo Markov Chain sampling}. Right now we can ignore what exactly that is and ``just do it''. 

The JAGS model will have the following components:

\begin{enumerate}
\item The \textbf{model specification}: This will be a model written as a text file from R, using the \texttt{cat} command.
\item The \textbf{data specification}: This must be a list, see example below. 
\item Define which parameters you want to look at the posterior distribution of; we will conventionally use a vector called \texttt{track.variables} to keep track of these.
\item Run the \texttt{jags.model} function with parameters and save the result as an object, say \texttt{normalex1.mod}. Conventionally we will use the extension .mod for model objects.

\begin{verbatim}
jags.model( 
    ## your data as a list:
    data = data,
    ## model location in hard drive:
    file = "JAGSmodels/normalexercise1.jag",
    ## ignore all this right now:
    n.chains = 4,
    n.adapt =2000 ,quiet=T)
\end{verbatim}

\item Run the command \texttt{coda.samples} using the following syntax, and save the result as say \texttt{normalex1.res}. We will conventionally save results with the extension .res.

\begin{verbatim}
normalex1.res<-coda.samples( normalex1.mod,
              var = track.variables,
              ## ignore all this for now:
              n.iter = 100000,
              thin = 50 ) 
\end{verbatim}

\item Plot posterior distribution using 

\begin{verbatim}
plot(normalex1.res)
\end{verbatim}

\item Print out summary of posterior distribution using

\begin{verbatim}
summary(normalex1.res)
\end{verbatim}
\end{enumerate}

So here we go:

<<>>=
## model specification:
cat("
model
   {
for(i in 1:5){
    ## note specification in terms of precision:
  y[i] ~ dnorm(mu[i],1/0.6)
  mu[i] <- beta0
}
  ##prior
  beta0 ~ dnorm(14,1)
}",
    file="JAGSmodels/normalexercise1.jag" )

## data:
data<-list(y=c(15.7,14.7,16.1,16.2,15.1))

## specify variables to track
## the posterior distribution of:
track.variables<-c("beta0")

## load rjags library:
library(rjags,quietly=T)

## define model:
normalex1.mod <- jags.model( 
    data = data,
    file = "JAGSmodels/normalexercise1.jag",
    n.chains = 4,
    n.adapt =2000 ,quiet=T)

## run model:
normalex1.res <- coda.samples( normalex1.mod,
                                      var = track.variables,
                                      n.iter = 100000,
                                      thin = 50 ) 

## summarize and plot:
summary(normalex1.res)

plot(normalex1.res)

## convergence diagnostic:
## (to be explained later)
gelman.diag(normalex1.res)
@

We can visualize the prior, likelihood, posterior:

<<fig=F,label=priorpostlik>>=
theta<-seq(0,18,by=0.01)

## lik: the sampling distribution of the mean:
plot(function(x) dnorm(x,mean=mean(data$y),sd=sqrt(0.6/5)), 
     12, 18,
     ylab="density",xlab="X",ylim=c(0,1.3))

## prior
lines(theta,dnorm(theta,mean=14,sd=1),lty=2)

#posterior from JAGS:
## mean=15.40056, sd=0.32127
lines(theta,dnorm(theta,mean=15.40056,sd=0.32127),col="red")
@


\begin{figure}  
<<fig=T,echo=F>>=
<<priorpostlik>>
@
\caption{Visualization of prior, likelihood, posterior.}\label{fig:}
\end{figure}

\textbf{Final exercise}: Visualize the prior, likelihood and posterior for v=0.001, v=100. Obtain the posterior distribution's parameters using both the formula and by generating the posterior sample using (modified) JAGS code.



