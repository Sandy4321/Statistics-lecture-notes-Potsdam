
\chapter{Maximum likelihood estimation}

Here, we look at the sample values and then choose as our estimates of the unknown parameters the values for which the probability or probability density of getting the sample values is a maximum. 

\section{Discrete case}

Suppose the observed sample values are $x_1, x_2,\dots, x_n$. The probability of getting them is

\begin{equation}
P(X_1=x_1,X_2=x_2,\dots,X_n=x_n) = f(X_1=x_1,X_2=x_2,\dots,X_n=x_n;\theta)  
\end{equation} 

\noindent
i.e., the function $f$ is the value of the joint probability \textbf{distribution} of the random variables $X_1,\dots,X_n$ at $X_1=x_1,\dots,X_n=x_n$. Since the sample values have been observed and are fixed, $f(x_1,\dots,x_n;\theta)$ is a function of $\theta$. The function $f$ is called a \textbf{likelihood function}.

\section{Continuous case}

Here, $f$ is the joint probability \textbf{density}, the rest is the same as above.

\begin{definition}\label{def:lik}
If $x_1, x_2,\dots, x_n$ are the values of a random sample from a population with parameter $\theta$, the \textbf{likelihood function} of the sample is given by 

\begin{equation}
L(\theta) = f(x_1, x_2,\dots, x_n; \theta)  
\end{equation}

\noindent
for values of $\theta$ within a given domain. Here, $f(X_1=x_1,X_2=x_2,\dots,X_n=x_n;\theta)$ is the joint probability distribution or density of the random variables $X_1,\dots,X_n$ at $X_1=x_1,\dots,X_n=x_n$.

\end{definition}

So, the method of maximum likelihood consists of maximizing the likelihood function with respect to $\theta$. The value of $\theta$ that maximizes the likelihood function is the \textbf{MLE} (maximum likelihood estimate) of $\theta$.


\section{Finding maximum likelihood estimates for different distributions}

\paragraph{Example 1}

Let $X_i$, $i=1,\dots,n$ be a random variable with PDF $f(x; \sigma) = \frac{1}{2\sigma} exp (-\frac{\mid x \mid}{\sigma})$. Find $\hat \sigma$, the MLE of $\sigma$.


\begin{equation}
	L(\sigma) = \prod f(x_i; \sigma) = \frac{1}{(2\sigma)^n} exp (-\sum \frac{\mid x_i \mid}{\sigma})
\end{equation}

Let $\ell$ be log likelihood. Then:

\begin{equation}
	\ell (x; \sigma) = \sum \left[ - \log 2 - \log \sigma - \frac{\mid x_i \mid}{\sigma} \right]
\end{equation}

Differentiating and equating to zero to find maximum:

\begin{equation}
	\ell ' (\sigma) = \sum \left[- \frac{1}{\sigma} + \frac{\mid x_i \mid}{\sigma^2}  \right] = - \frac{n}{\sigma} + \frac{\mid x_i \mid}{\sigma^2} =
	 0
\end{equation}

Rearranging the above, the MLE for $\sigma$ is:

\begin{equation}
	\hat \sigma = \frac{\sum \mid x_i \mid}{n}
\end{equation}

\paragraph{Example 2: Exponential}


\begin{equation}
	f(x; \lambda)= \lambda exp (- \lambda x)
\end{equation}

Log likelihood:

\begin{equation}
	\ell = n \log \lambda - \sum \lambda x_i
\end{equation}

Differentiating:

\begin{equation}
	\ell ' (\lambda) = \frac{n}{\lambda} - \sum x_i = 0
\end{equation}

\begin{equation}
	\frac{n}{\lambda} =  \sum x_i
\end{equation}

I.e., 

\begin{equation}
	\frac{1}{\hat \lambda} =  \frac{\sum x_i}{n}
\end{equation}


\paragraph{Example 3: Poisson}

\begin{eqnarray}
	L (\mu; x) & = \prod \frac{\exp^{-\mu} \mu ^{x_i}}{x_i!}\\
	           & = \exp^{-\mu} \mu^{\sum x_i} \frac{1}{\prod x_i !} 
\end{eqnarray}


Log lik:

\begin{equation}
\ell (\mu; x) = -n\mu + \sum x_i \log \mu - \sum \log y!	
\end{equation}

Differentiating:

\begin{equation}
\ell ' (\mu) = -n + \frac{\sum x_i}{\mu}	= 0
\end{equation}

Therefore:

\begin{equation}
\hat \lambda = \frac{\sum x_i}{n}
\end{equation}


\paragraph{Example 4: Binomial}



\begin{equation}
L(\theta) = {n \choose x} \theta^x (1-\theta)^{n-x}	
\end{equation}

Log lik:

\begin{equation}
\ell (\theta) = \log {n \choose x} + x \log \theta + (n-x)	\log (1-\theta)
\end{equation}

Differentiating:

\begin{equation}
\ell ' (\theta) = \frac{x}{\theta} - \frac{n-x}{1-\theta} = 0	
\end{equation}

Thus:

\begin{equation}
\hat \theta = \frac{x}{n}	
\end{equation}


\paragraph{Example 5: Normal}

Let $X_1,\dots,X_n$ constitute a random variable of size $n$ from a normal population with mean $\mu$ and variance $\sigma^2$, find joint maximum likelihood estimates of these two parameters.

\begin{eqnarray}
L(\mu; \sigma^2) & = \prod N(x_i; \mu, \sigma)	\\
                 & = (\frac{1}{2 \pi\sigma^2 })^{n/2} \exp (-\frac{1}{2\sigma^2} \sum (x_i - \mu)^2)\\ 
\end{eqnarray}


Taking logs and differentiating with respect to $\mu$ and $\sigma^2$, we get:

\begin{equation}
	\hat \mu = \frac{1}{n}\sum x_i = \bar{x}	
\end{equation}

and

\begin{equation}
	\hat \sigma ^2 = \frac{1}{n}\sum (x_i-\bar{x})^2
\end{equation}
 

%Note that we did not show that $\hat\sigma$ is an MLE of $\sigma$. But MLEs have the invariance property: if $ \hat \Theta$ is a maximum likelihood estimator of $\theta$, and the function $g(\hat \Theta)$ is continuous, then $g(\hat \Theta)$  is also an ML estimator of $g(\theta)$. 

\paragraph{Example 6: Geometric}

\begin{equation}
f(x; p) = (1-p)^{x-1} p	
\end{equation}

\begin{equation}
L(p) = p ^ n (1-p)^{\sum x - n}
\end{equation}

Log lik:

\begin{equation}
\ell (p) = n \log p + (\sum x -n ) \log (1-p)
\end{equation}

Differentiating:

\begin{equation}
\ell ' (p)	\frac{n}{p} - \frac{\sum x - n }{1-p} = 0
\end{equation}


\begin{equation}
\hat p = \frac{1}{\bar{x}}	
\end{equation}

\section{Visualizing likelihood and maximum log likelihood for normal}

For simplicity consider the case where $N(\mu=0,\sigma^2=1)$.

<<logliknormal>>=
op<-par(mfrow=c(1,2),pty="s")
plot(function(x) dnorm(x,log=F), -3, 3,
      main = "Normal density",#ylim=c(0,.4),
              ylab="density",xlab="X")
abline(h=0.4)
plot(function(x) dnorm(x,log=T), -3, 3,
      main = "Normal density (log)",#ylim=c(0,.4),
              ylab="density",xlab="X")
abline(h=log(0.4))
@

\begin{marginfigure}
<<fig=T,echo=F>>=
<<logliknormal>>
@
\caption{Maximum likelihood and log likelihood.}\label{fig:maxlik}
\end{marginfigure}

\section{Obtaining standard errors}

Once we have found a maximum likelihood estimate, say of $p$ in the binomial distribution, we need a way to a quantify our uncertainty about how good $\hat p$ is at estimating the population parameter $p$.

The second derivative of the log likelihood gives you an estimate of the variance of the sampling distribution of the sample mean (SDSM).

Here is a sort-of explanation for why the second derivative does this.
The second derivative is telling us the rate at which the rate of change is happening in the slope, i.e., the rate of curvature of the curve.
When the variance of the SDSM is low, then we have a sharp rate of change in slope (high value for second derivative), and so if we take the inverse of the second derivative, we get a small value, an estimate of the low variance.
And when the variance is high, we have a slow rate of change in slope (slow value for second derivative), so if we invert it, we get a large value.

<<ratesofchange>>=
op<-par(mfrow=c(1,2),pty="s")

plot(function(x) dnorm(x,log=F,sd=0.001), -3, 3,
      main = "Normal density",#ylim=c(0,.4),
              ylab="density",xlab="X")
plot(function(x) dnorm(x,log=F,sd=10), -3, 3,
      main = "Normal density",#ylim=c(0,.4),
              ylab="density",xlab="X")
@

\begin{marginfigure}
<<fig=T,echo=F>>=
<<ratesofchange>>
@
\caption{How variance relates to the second derivative.}\label{fig:ratesofchange}
\end{marginfigure}

Notice that all these second derivatives would be negative, because we are approaching a maximum as we reach the peak of the curve. So when we take an inverse to estimate the variances, we get negative values. It follows that if we were to take a negative of the inverse, we'd get a positive value. 

This is the reasoning that leads to the following steps for computing the variance of the SDSM:

\begin{enumerate}
\item 
Take the second partial derivative of the log-likelihood. 
\item 
Compute the negative of the expectation of the second partial derivative. This is called the Information Matrix $I(\theta)$.
\item 
Invert this matrix to obtain estimates of the variances and covariances. To get standard errors take the square root of the diagonal elements in the matrix.
\end{enumerate}

It's better to see this through an example:

\paragraph{Example 1: Binomial}


Instead of calling the parameter $\theta$ I will call it $p$.

\begin{equation}
L(p) = {n \choose x} p^x (1-p)^{n-x}  
\end{equation}

Log lik:

\begin{equation}
\ell (p) = \log {n \choose x} +  x \log p + (n-x)	\log (1-p)
\end{equation}

Differentiating:

\begin{equation}
\ell ' (p) = \frac{x}{p} - \frac{n-x}{1-p} = 0	
\end{equation}

Taking the second partial derivative with respect to p:

\begin{equation}
\ell '' (p) = -\frac{x}{p^2} - \frac{n- x}{(1-p)^2} 
\end{equation}

The quantity $-\ell '' (p)$ is called \textbf{observed Fisher information}.

Taking expectations:

\begin{equation}
E(\ell '' (p)) = E(-\frac{x}{p^2} - \frac{n- x}{(1-p)^2} )  
\end{equation}



Exploiting that fact the $E(x/n)=p$ and so $E(x)=E(n\times x/n)=np$, we get


\begin{equation}
E(\ell '' (p)) = E(-\frac{x}{p^2} - \frac{n- x}{(1-p)^2} )  = - \frac{np}{p^2}-\frac{n-np}{(1-p)^2} \explain{=}{exercise} -\frac{n}{p(1-p)} 
\end{equation}

Next, we negate and invert the expectation:

\begin{equation}
-\frac{1}{E(\ell '' (\theta))}=\frac{p(1-p)}{n}
\end{equation}

Evaluating this at $\hat p$, the estimated value of the parameter, we get:

\begin{equation}
-\frac{1}{E(\ell '' (\theta))}=\frac{\hat p(1-\hat p)}{n} = \frac{1}{I(p)}
\end{equation}

[Here, $I(p)$ is called \textbf{expected Fisher Information}.]

If we take the square root of the inverse Information Matrix

\begin{equation}
\sqrt{\frac{1}{I(p)}} = \sqrt{\frac{\hat p(1-\hat p)}{n}}
\end{equation}

we have the \textbf{estimated standard error}. 

Another example using the normal distribution:

\paragraph{Example 2: Normal distribution}


This example is based on Khuri\cite{khuri2003advanced} (p.\ 309). Let 
$X_1,\dots,X_n$ be a sample of size $n$ from $N(\mu,\sigma^2)$, both parameters of the normal unknown. 

\begin{equation}
L(x\mid \mu, \sigma^2)= 
\frac{1}{(2\pi \sigma^2)^{n/2}} \exp[-\frac{1}{2\sigma^2} \sum (x-\mu)^2]
\end{equation}


Taking log likelihood:

\begin{equation}
\ell = -\frac{n}{2} \log \frac{1}{(2\pi \sigma^2)}-
\frac{1}{2\sigma^2} \sum (x-\mu)^2
\end{equation}

Taking partial derivatives with respect to $\mu$ and $\sigma^2$ we have:

\begin{equation}
\frac{\partial \ell}{\partial \mu} = \frac{1}{\sigma^2} \sum (x-\mu) = 0 \Rightarrow n(\bar{x}-\mu)= 0
\end{equation}

\begin{equation}
\frac{\partial \ell}{\partial \sigma^2} 
= \frac{1}{2\sigma^4} \sum (x-\mu)^2 - \frac{n}{2\sigma^2} = 0 \Rightarrow \sum (x-\mu)^2 - n\sigma^2 = 0
\end{equation}

Simplifying we get the maximum likelihood estimates of $\mu$ and $\sigma^2$: $\hat \mu= \bar{x}$ and $\hat \sigma^2 = \frac{1}{n}\sum (x-\bar{x})^2$. Note that these are unique values.

We can verify that $\hat \mu$ and $\hat \sigma^2$ are the values of $\mu$ and $\sigma^2$ that maximize $L(x\mid \mu,\sigma^2)$.
This can be done by taking the second order partial derivatives and finding out whether we are at a maxima or not. It is convenient to write the four partial derivatives in the above example as a matrix, and this matrix is called a Hessian matrix. If this matrix is positive definite (i.e., if the determinant\footnote{Suppose a a matrix represents a system of linear equations, as happens in linear modeling. A determinant of a matrix tells us whether there is a unique solution to this system of equations; when the determinant is non-zero, there is a unique solution. Given a matrix 

$\begin{pmatrix}
a & b \\
c & d\\
\end{pmatrix}$

the determinant is $ad-bc$. In this course, we don't need to know much about the determinant. This is the only place in this course that this term turns up.} 
of the matrix is greater than 0), we are at a maximum. 

The Hessian is also going to   
lead us to the information matrix as in the previous example: we just take the negative of the expectation of the Hessian, and invert
it to get the variance covariance matrix. (This is just like in the binomial example above, except that we have two parameters to worry about rather than one.)\footnote{
The inverse of a matrix:

$\begin{pmatrix}
a & b \\
c & d\\
\end{pmatrix}^{-1}
= \frac{1}{ad-bc} 
\begin{pmatrix}
d & -b \\
-c & a\\
\end{pmatrix}$
} 

Consider the Hessian matrix $H$ of the second partial derivatives of the log likelihood $\ell$.

\begin{equation}
H = \begin{pmatrix}
\frac{\partial^2 \ell}{\partial \mu^2} & \frac{\partial^2 \ell}{\partial \mu\partial \sigma^2}\\
\frac{\partial^2 \ell}{\partial \mu\partial \sigma^2} & 
\frac{\partial^2 \ell}{\partial \sigma^4} \\
\end{pmatrix}
\end{equation}

Now, if we compute the second-order partial derivatives replacing $\mu$ with $\hat \mu$ and $\sigma^2$ with $\hat \sigma^2$ (i.e., the values that we claim are the MLEs of the respective parameters), we will get:
\begin{equation}
\frac{\partial^2 \ell}{\partial \mu^2}=
-\frac{n}{\hat \sigma^2}
\end{equation}

\begin{equation}
\frac{\partial^2 \ell}{\partial \mu\partial \sigma^2} = -\frac{1}{\hat \sigma^2}\sum (x - \hat\mu) = 0
\end{equation}

\begin{equation}
\frac{\partial^2 \ell}{\partial \sigma^4}= -\frac{n}{2\hat \sigma^2}
\end{equation}

The determinant of the Hessian is $\frac{n^2}{2\hat \sigma^6}>0$. Hence, $(\hat \mu, \hat \sigma^2)$ is a point of local maximum of $\ell$. Since it's the only maximum (we established that when we took the first derivative), it must also be the absolute maximum.

As mentioned above, if we take the negation of the expectation of the Hessian, we get the Information Matrix, and if we invert the Information Matrix, we get the variance-covariance matrix.

Once we take the negation of the expectation, we get ($\theta=(\mu,\sigma^2)$):

\begin{equation}
I(\theta)= 
\begin{pmatrix}
\frac{n}{\sigma^2} & 0 \\
0 & \frac{n}{2\sigma^4}
\end{pmatrix}
\end{equation}

Finally, if we take the inverse and evaluate it at the MLEs, we will get:

\begin{equation}
\frac{1}{I(\theta)}= 
\begin{pmatrix}
\frac{\hat \sigma^2}{n} & 0 \\
0 & \frac{2\hat \sigma^4}{n}
\end{pmatrix}
\end{equation}

And finally, if we take the square root of each element in the matrix, we get the estimated standard error of $\hat \mu$ to be 
$\frac{\hat \sigma}{\sqrt{n}}$, and the standard error of the $\hat \sigma^2$ to be 
$\hat \sigma^2 \sqrt{2/n}$. The estimated standard error of the sample mean should look familiar!

I know that this is heavy going; luckily for us, for our limited purposes we can always let R do the work, as I show below. 

\section{MLE using R}

\paragraph{One-parameter case}

\textbf{Example 1: Estimating $\lambda$ in the Box-Cox transform}

Let's assume that there exists a $\lambda$ such that 

\begin{equation}
f_\lambda (y_i) = x_i^T \beta + \epsilon_i \quad \epsilon_i \sim N(0, \sigma^2)
\end{equation}

We use maximum likelihood estimation to estimate $\lambda$. Note that

$L(\beta_\lambda, \sigma^2_\lambda, \lambda; y) \propto$

\begin{equation}
(\frac{1}{\sigma})^n \exp [-\frac{1}{2\sigma^2} \sum [f_\lambda(y_i)-  x_i^T \beta ]^2] [\prod \explain{f'_\lambda(y_i)}{\hbox{Jacobian}}] 
\end{equation}

For fixed $\lambda$, we estimate $\hat{\beta}$ and $\hat{\sigma}^2$ in the usual MLE way, and then we turn our attention to $\lambda$:

\begin{equation}
L(\hat{\beta}_\lambda, \hat{\sigma}^2_\lambda, \lambda; y) = S_\lambda^{-n/2}\prod f'_\lambda(y_i) 
\end{equation}

Taking logs:

\begin{equation}
\ell = c-\frac{n}{2} \log S_\lambda + \sum \log f'_\lambda(y_i)
\end{equation}

One interesting case is the \textbf{Box-Cox family}.The relevant paper is by Box and Cox.\cite{box1964analysis} (An interesting side-note is that one reason that Box and Cox co-wrote that paper is that they thought it would be cool to have a paper written by two people called Box and Cox. At least that's what Box wrote in his autobiography, which is actually quite interesting to read.\cite{boxautobio})

\begin{equation}
f_\lambda (y) = \left\{ 
\begin{array}{l l}
       \frac{y^\lambda - 1}{\lambda}   & \lambda \neq 0\\
       \log y & \quad \lambda=0\\
\end{array}
\right.
\end{equation}

We assume that $f_\lambda (y) \sim N(x_i^T \beta,\sigma^2)$. So we have to just estimate $\lambda$ by MLE, along with $\beta$.
Here is how to do it by hand:

Since $f_\lambda=\frac{y^\lambda-1}{\lambda}$, it follows that $f'_\lambda(y)= y^{\lambda-1}$.

Now, for different $\lambda$ you can figure out the log likelihoods by hand by solving this equation:

\begin{equation}
\ell = c-\frac{n}{2} \log \explain{S_\lambda}{\hbox{Residual sum of squares}} + (\lambda-1)\sum \log (y_i)
\end{equation}


Next, we do this maximum likelihood estimation using R.

<<>>=
data<-rnorm(100,mean=500,sd=50)

bc<-function(lambda,x=data){
  if(lambda==0){sum(log(x))}
  if(lambda!=0){sum(log((x^lambda-1)/lambda))}
}
@

<<>>=
(opt.vals.default<-optimize(bc,interval=c(-2,2)))
@

\textbf{Example 2: Estimating $p$ for the binomial distribution}

A second example is to 
try doing MLE for the binomial. Let's assume we have the result of 10 coin tosses. We know that the MLE is the number of successes divided by the sample size:

<<>>=
x<-rbinom(10,1,prob=0.5) 
sum(x)/length(x)
@  

We will now get this number using MLE. We do it numerically to illustrate the principle. First, we define a negative log likelihood function for the binomial. Negative because the function we will use to optimize does minimization by default, so we just flip the sign on the log likelihood.

<<>>=
negllbinom <- function(p, x){ 
  -sum(dbinom(x, size = 1, prob = p,log=T)) 
}
@

Then we run the optimization function:

<<>>=
optimize(negllbinom, 
         interval = c(0, 1), 
         x = x) 
@

\paragraph{Two-parameter case}

Here is an example of MLE using R. Note that in this example, we could have analytically figured out the MLEs. Instead, we are doing this numerically. The advantage of the numerical approach becomes obvious  when the analytical way is closed to us. 

Assume that you have some data that was generated from a normal distribution, with mean 500, and standard deviation 50. Let's say you have 100 data points.

<<>>=
data<-rnorm(100,mean=500,sd=50)
@

Let's assume we don't know what the mean and standard deviation are. 
Now, of course you know how to estimate these using the standard formulas. 
But right now we are going to estimate them using MLE. 

We first write down the negation of the log likelihood function. We take the negation because the optimization function we will be using (see below) does minimization by default, so to get the maximum with the default setting, we just change the sign. 

The function \texttt{nllh.normal} takes a vector \texttt{theta} of parameter values, and a data frame \texttt{data}. 

<<>>=
nllh.normal<-function(theta,data){ 
  ## decompose the parameter vector to
  ## its two parameters:
  m<-theta[1] 
  s<-theta[2] 
  ## read in data
  x <- data
  n<-length(x) 
  ## log likelihood:  
  logl<- sum(dnorm(x,mean=m,sd=s,log=TRUE))
  ## return negative log likelihood:
  -logl
  }
@

Here is the negative log lik for mean = 40, sd 4, and for mean = 800 and sd 4:

<<>>=
nllh.normal(theta=c(40,4),data)

nllh.normal(theta=c(800,4),data)
@

As we would expect, the negative log lik for mean 500 and sd 50 is much smaller (due to the sign change) than the two log liks above:

<<>>=
nllh.normal(theta=c(500,50),data)
@

Basically, you could sit here forever, playing with combinations of values for mean and sd to find the combination that gives the optimal log likelihood. R has an optimization function that does this for you. We have to specify some sensible starting values:

<<>>=
opt.vals.default<-optim(theta<-c(700,40),nllh.normal,
      data=data,
      hessian=TRUE)
@

Finally, we print out the estimated parameter values that maximize the likelihood:

<<>>=
(estimates.default<-opt.vals.default$par)
@

And we compute standard errors by taking the square root of the diagonals of the Hessian computed by the optim function:

<<>>=
(SEs.default<-sqrt(diag(solve(opt.vals.default$hessian))))
@

These values match our standard calculations:

<<>>=
## compute estimated standard error from data:
sd(data)/sqrt(100)
@

\subsection{Using optim in the one-parameter case}

Note that we could not get the Hessian in the one-parameter case using optimize (for the binomial). We can get the $1\times 1$ Hessian by using optim in the binomial case, but we have to make sure that our syntax is correct. 

Note that optim takes a vector of parameter names. That's the key insight. One other minor detail (for us) is that the optimization method has to be specified when using optim for one parameter optimization.

<<>>=
negllbinom2 <- function(theta){
   p<-theta[1]
   -sum(dbinom(x, size = 1, prob = p,log=T)) 
}

## testing:
negllbinom2(c(.1))

opt.vals.default<-optim(theta<-c(.5),
                        negllbinom2,
      method="Brent",                  
      hessian=TRUE,lower=0,upper=1)

## SE:
sqrt(solve(opt.vals.default$hessian))
@

The estimated SE matches our analytical result earlier:

\begin{equation}
\sqrt{\frac{1}{I(p)}} = \sqrt{\frac{\hat p(1-\hat p)}{n}}
\end{equation}

This calculation and the number that the optimization procedure delivers coincide, up to rounding error:

<<>>=
sqrt((0.4 * (1-0.4))/10)
@

%%to-do:
%%http://zoonek.free.fr/blosxom/R/2012-06-01_Optimization.html