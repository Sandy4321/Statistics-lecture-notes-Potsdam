
\chapter{Introduction}

\section{The aims of this course}

I'm assuming that you know how to fit and interpret linear models and linear mixed models for analyzing psycholinguistic data. A minimal understanding of the underlying statistical theory is assumed; this is quite a modest requirement: the course I teach in summer is all the background I assume.

It feels like it's time psycholinguists made the leap to using bayesian methods as a standard tool for inference. For the kind of work most of us do, using bayesian tools is very much within our reach. 
This course will cover some of the key aspects of the bayesian approach to data analysis that are relevant to people who fit linear mixed models on repeated measures data.

\section{Software needed}

Apart from R, we will need JAGS (http://mcmc-jags.sourceforge.net/); and within R we will need the \texttt{rjags} package.
Both were developed by the epidemiologist/statistician Martyn Plummer. 
JAGS and rjags are installed in the CIP pool in Haus 14, but you should install them on your own machines too. You might need some other packages; this will become clear as you work through the notes.

The best tutorial I have seen for setting up software for using JAGS is:

\begin{verbatim}
http://spot.colorado.edu/~joka5204/bayes2013.html
\end{verbatim}

Just follow the instructions provided there to set yourself up.

\section{How to read these notes}

I am expecting that most people attending the course do not know calculus (or have forgotten it). This course is intended to be more of a practical introduction than a theoretical one. I will therefore walk you through the more technical parts; at no point do you \textbf{have} to do any analysis using 
calculus. Your goal should be to just understand the general idea; I will try to facilitate that process.

The big barrier in understanding bayesian methods is that you need to understand some of the theory. With frequentist methods, you often can get by just knowing ``which button to press'' (or, at least, neither you nor your reviewer or your audience will ever know that anything went wrong). In Bayesian methodology it's important to know a bit about the what and why.
I will try to minimize the mathematical load.
It's probably easiest to read these notes (especially the first half) \textbf{after} hearing my lectures. 

Finally, I should mention that I am still, at the time of writing this (2013), relatively new to bayesian methods, so don't be disappointed if you find that I don't know everything.

These notes are supposed to be self-contained, but I do point the reader to books on bayesian methods. All this reading is strictly optional.

\section{Short review of linear (mixed) models}

Our focus here is on linear mixed models, because this is our bread and butter in psycholinguistics.
In this section our only goal is to have some idea of what the components of a linear (mixed) model are. Basically, we need to know how to talk about the variance components and the `fixed' part because we will need to fit bayesian models using this specification.

The basic linear model that we know how to fit in R has the following linear relationship between the response variable y and the predictor (matrix) X:

\begin{equation}
y=X\beta+\epsilon
\end{equation}

where

\begin{tabular}{@{}ll@{}ll@{}}
$E(y) = X\beta = \mu$ &  & $E(\epsilon)=0$  \\
$Var(y) = \sigma^2 I_n $ & & $Var(\epsilon) = \sigma^2 I_n$ \\
\end{tabular}

I will spell this out a bit in class.

In linear modelling we model the mean of a response $y_1,\dots,y_n$ as a function of a vector of predictors $x_1,\dots,x_n$. We assume that the $y_i$ are conditionally independent given $X, \beta$. When $y$'s are not marginally independent, we have $Cor(y_1,y_2)\neq 0$, or $P(y_2\mid y_1)\neq P(y_2)$.

Linear mixed models are useful for correlated data where $\mathbf{y}\mid X, \beta$ are not independently distributed. 

%\begin{equation}
%P(Y_1,\dots,Y_n\mid x_1,\dots,x_n,\beta) = 
%\prod P(Y_i,\dots,x_i,\beta)
%\end{equation}


\paragraph{Basic specification of LMMs}

\begin{equation}
\explain{Y_i}{n\times 1} = \explain{X_i}{n\times p}~~\explain{\beta}{p\times 1} + \explain{Z_i}{n\times q}~~\explain{b_i}{q\times 1} + \explain{\epsilon_i}{n\times 1}
\end{equation}

where $i=1,\dots,m$, let $n$ be total number of data points.

Distributional assumptions:

$ b_i \sim N(0,D)$ and $\epsilon_i \sim N(0,\sigma^2 I)$. $D$ is a $q\times q$ matrix that does not depend on $i$, and $b_i$ and $\epsilon_i$ are assumed to be independent.

$Y_i$ has a multivariate normal distribution:

\begin{equation}
Y_i \sim N(X_i \beta, V(\alpha))
\end{equation}

where $V(\alpha)=Z_i D Z_i^T+\sigma^2 I$, and $\alpha$ is the variance component parameters.

Note:
\begin{enumerate}
\item
D has to be symmetric and positive definite. 
\item
The $Z_i$ matrix columns are a subset of $X_i$. In the random intercept model, $Z_i = 1_i$.
\item
In the varying intercepts and varying slopes model, $X_i = Z_i = (1_i, X_i)$. Then:

\begin{equation}
Y_i = X_i (\beta+b_i) +\epsilon_i
\end{equation}

or

\begin{equation}
Y_i = X_i \beta_i +\epsilon_i \quad \beta_i \sim N(\beta,D)
\end{equation}

\begin{equation}
\begin{split}
D=&
\begin{pmatrix}
d_{00} & d_{01}\\
d_{10} & d_{11}\\
\end{pmatrix}\\
=& 
\begin{pmatrix}
d_{00}=Var(\beta_{i0}) & d_{01}=Cov(\beta_{i0},\beta_{i1})\\
d_{10}=Cov(\beta_{i0},\beta_{i1}) & d_{11}=Var(\beta_{i1})\\
\end{pmatrix}
\end{split}
\end{equation}


\end{enumerate}

\section{$\sigma_b^2$ describes both between-block variance, and within block covariance}

Consider the following model, a varying intercepts model:

\begin{equation}
Y_{ij} = \mu + b_i + e_{ij},
\end{equation}


with $b_i\sim N(0,\sigma^2_b)$, $e_{ij}\sim N(0,\sigma^2)$.

Note that variance is a covariance of a random variable with itself, and then consider the model formulation. If we have

\begin{equation}
Y_{ij} = \mu + b_i + \epsilon_{ij}
\end{equation}

where i is the group, j is the replication, if we \textit{define} $b_i\sim N(0, \sigma^2_b)$, and refer to $\sigma^2_b$ as the between group variance, then we must have


\begin{equation}
\begin{split}
Cov(Y_{i1}, Y_{i2}) =& Cov(\mu + b_i + \epsilon_{i1} , \mu + b_i + \epsilon_{i2})\\
=& \explain{Cov(\mu, \mu)}{=0} + 
 \explain{Cov(\mu, b_i)}{=0} +
 \explain{Cov(\mu, \epsilon_{i2})}{=0}+ \\
 ~~& \explain{Cov(b_i,\mu)}{=0} +
  \explain{Cov(b_i,b_i)}{+ve} \dots\\
  =&  Cov(b_i, b_i) = Var(b_i) = \sigma^2_b\\
\end{split}
\end{equation}

\section{Two basic types of linear mixed model and their variance components}

\subsection{Varying intercepts model}

\begin{fmpage}{\linewidth}
The model for a categorical predictor is:

\begin{equation}
Y_{ijk} = \beta_j + b_{i}+\epsilon_{ijk}
\end{equation}
\end{fmpage}

\noindent
$i=1,\dots,10$ is subject id, $j=1,2$ is the factor level, $k$ is the number of replicates (here 1).
$b_i \sim N(0,\sigma_b^2), \epsilon_{ijk}\sim N(0,\sigma^2)$.

\begin{fmpage}{\linewidth}
For a continuous predictor:

\begin{equation}
Y_{ijk} = \beta_0 + \beta_1 t_{ijk} + b_{ij} +\epsilon_{ijk}
\end{equation}
\end{fmpage}

The general form for any model in this case is:

\begin{equation}
\begin{pmatrix}
Y_{i1}\\
Y_{i2}
\end{pmatrix}
\sim
N\left(
\begin{pmatrix}
\beta_1\\
\beta_2\\
\end{pmatrix}
,
V
\right)
\end{equation}

where 

\begin{equation}
V =\begin{pmatrix}
\sigma_b^2 + \sigma^2 & \sigma_b^2\\
\sigma_b^2 & \sigma_b^2 + \sigma^2\\
\end{pmatrix}
=
\begin{pmatrix}
\sigma^2_{b} + \sigma^2  &  \rho\sigma_{b}\sigma_{b}\\
\rho\sigma_{b}\sigma_{b} & \sigma^2_{b}+\sigma^2  \\       
\end{pmatrix}
\end{equation}

Note also that the mean response for the subject, i.e., \textit{conditional} mean of $Y_{ij}$ given the subject-specific effect $b_i$ is:

\begin{equation}
E(Y_{ij}\mid b_i)= X_{ij}^T \beta + b_i
\end{equation}

The mean response in the population, i.e., the marginal mean of $Y_{ij}$:

\begin{equation}
E(Y_{ij})= X_{ij}^T \beta
\end{equation}

The marginal variance of each response is:

\begin{equation}
\begin{split}
Var(Y_{ij})=& Var(X_{ij}^T \beta+b_i + \epsilon_{ij})\\
 =& Var(\beta+b_i + \epsilon_{ij})\\
 =& \sigma_b^2 + \sigma^2\\
\end{split}
\end{equation}

the covariance between any pair of responses $Y_{ij}$ and $Y_{ij'}$ is given by

\begin{equation}
\begin{split}
Cov(Y_{ij},Y_{ij'}) =& Cov(X_{ij}^T\beta + b_i + \epsilon_{ij},X_{ij'}^T\beta + b_i + \epsilon_{ij'})\\
=& Cov(b_i + e_{ij},b_i + e_{ij'})\\
=& Cov(b_i,b_i)=\sigma_b^2
\end{split}
\end{equation}

The correlation is

\begin{equation}
Corr(Y_{ij},Y_{ij'})= \frac{\sigma_b^2}{\sigma_b^2+\sigma^2}
\end{equation}

In other words, introducing a random intercept induces a correlation among repeated measurements.

$\hat{V}$ is therefore:

\begin{equation}
\begin{pmatrix}
\hat{\sigma}^2_{b} + \hat{\sigma}^2  &  \hat{\rho}\hat{\sigma}_{b}\hat{\sigma}_{b}\\
\hat{\rho}\hat{\sigma}_{b}\hat{\sigma}_{b} & \hat{\sigma}^2_{b}+\hat{\sigma}^2  \\       
\end{pmatrix}
\end{equation}

Note: $\hat{\rho}=1$. But this correlation is not \textit{estimated} in the varying intercepts model (note: I am not 100\% sure about this last sentence; I'm still working on understanding this part).

\subsection{Varying intercepts and slopes (with correlation)}

\begin{fmpage}{\linewidth}
The model for a categorical predictor is:

\begin{equation}
Y_{ij} = \beta_1+b_{1i}+(\beta_2+b_{2i})x_{ij}+\epsilon_{ij} \quad i=1,...,M, j=1,...,n_i
\end{equation}
\end{fmpage}

with $b_{1i}\sim N(0,\sigma_1^2), b_{2i}\sim N(0,\sigma_2^2)$, and $\epsilon_{ij}\sim N(0,\sigma^2)$.


\textit{Note: I have seen this presentation elsewhere}:

\begin{equation}
Y_{ijk} = \beta_j + b_{ij}+\epsilon_{ijk}
\end{equation}

\noindent
$b_{ij}\sim N(0,\sigma_b)$. The variance $\sigma_b$ must be a $2\times 2$ matrix:

\begin{equation}
\begin{pmatrix}
\sigma_1^2 & \rho \sigma_1 \sigma_2\\
\rho \sigma_1 \sigma_2 & \sigma_2^2\\
\end{pmatrix}
\end{equation}

The general form for the model is:

\begin{equation}
\begin{pmatrix}
Y_{i1}\\
Y_{i2}
\end{pmatrix}
\sim
N\left( 
\begin{pmatrix}
\beta_1\\
\beta_2\\
\end{pmatrix}
,
V
\right)
\end{equation}

where 

\begin{equation}
V =
%\begin{pmatrix}
%\sigma_b^2 + \sigma^2 & \sigma_b^2\\
%\sigma_b^2 & \sigma_b^2 + \sigma^2
%\end{pmatrix}=
\begin{pmatrix}
\sigma^2_{b,A} + \sigma^2  &  \rho\sigma_{b,A}\sigma_{b,B}\\
\rho\sigma_{b,A}\sigma_{b,B} & \sigma^2_{b,B}+\sigma^2  \\       
\end{pmatrix}
\end{equation}

Basically, when we fit LMMs in JAGS we will simply state the above model in BUGS syntax.

\paragraph{Example: Reading Time data}

This is a two-condition experiment, self-paced reading data kindly released by Gibson and Wu.\cite{gibsonwu} This dataset is unique because we will make a replication attempt in an exercise I will give out later in the course. The full analysis is discussed in my PLoS One paper\cite{VasishthetalPLoSOne2013}.

<<echo=F>>=
expt1 <- read.table("data/example1spr.txt")

colnames(expt1) <- c("subj","item","type",
                     "pos","word", "correct","rt")

data<-expt1[ , c(1, 2, 3, 4, 6, 7)]

## rename data frame
dat<-data

#head(dat)

## 37 subjects
#length(unique(dat$subj))
#xtabs(~subj+item,dat)
#length(sort(unique(dat$subj)))
@

<<label=gwpreproc,echo=FALSE>>=
questions <- subset(expt1,correct%in%c(1,0)) 
questions$correct <- as.numeric(as.character(questions$correct))

#subj-ext: c0 | c1 | c2 | c3 | say | V1 | N1 | DE | N2 | N2+1 | N2+2 | N2+3
#obj-ext : c0 | c1 | c2 | c3 | say | N1 | V1 | DE | N2 | N2+1 | N2+2 | N2+3

# rc started at pos 5; de was always at pos7

#       subj-ext  obj-ext   
#so          -1        1   effect of subject-object RC

de2<-cbind(subset(expt1,pos=="5"),region="de2") ## position de-2
de1<-cbind(subset(expt1,pos=="6"),region="de1") ## position de-1

## de2 and de1 are always contiguous, on successive lines. No missing data:
#dim(de2)
#dim(de1)

## sum up V+N and N+V sequences:
de12sum<-de1$rt+de2$rt

## add to de1 a column for the sum of de1 and d2 rt's.
## Note that this is a destructive step, the rt column is
## overwritten. I had to do this to be able to assemble the whole
## dataset into critdata below. Everything is recoverable by 
## rerunning the script.
de1$rt<-de12sum
de <- cbind(subset(expt1,pos=="7"),region="de")
hnoun <- cbind(subset(expt1,pos=="8"),region="headnoun")
## sum up de and noun RTs:
de.hnoun.rt<-de$rt+hnoun$rt
## make a copy of hnoun:
dehnoun<-hnoun
## replace rt column in hnoun with de.hnoun.rt:
dehnoun$rt<-de.hnoun.rt
## rename region
dehnoun$region<-"dehnoun"
hnoun1 <- cbind(subset(expt1,pos=="9"),region="headnoun1")

critdata <- rbind(de1,de,dehnoun,hnoun,hnoun1)

type2<-factor(ifelse(critdata$type=="obj-ext","object relative","subject relative"))
critdata$type2<-type2
@

<<echo=F>>=
library(lme4)
library(car)
library(ggplot2)

## treatment contrasts should be OK for analysis...
#contrasts(critdata$type)

## but let's do a regular anova style centering, sum contrasts:
critdata$so<- ifelse(critdata$type%in%c("subj-ext"),-0.5,0.5)

critdata<-critdata[,c(1,2,4,7,8,9,10)]
@

Here is the head of the data frame. Note that we have full crossing between subjects and items, but there is some missing data from subj 27 (no idea why):

<<>>=
head(critdata)
head(xtabs(~subj+item,critdata))
## subj 27:
xtabs(~subj+item,critdata)[24,]
@

We fit two LMMs (varying intercepts, and varying intercepts and slopes):

<<>>=
library(lme4)
## varying intercepts:
m0<-lmer(rt~so+(1|subj),subset(critdata,region=="headnoun"))

library(car)
qqPlot(residuals(m0))

qqPlot(ranef(m0)$subj)


## varying intercepts and slopes:
m1<-lmer(rt~so+(1+so|subj),subset(critdata,region=="headnoun"))

VarCorr(m1)


@

The varying intercepts model can be written as:

\begin{equation}
Y_{ijk} = \beta_j + b_{i}+\epsilon_{ijk}
\end{equation}

\noindent
$i=1,\dots,10$ is subject id, $j=1,2$ is the factor level, $k$ is the number of replicates.
$b_i \sim N(0,\sigma_b^2), \epsilon_{ijk}\sim N(0,\sigma^2)$.

The varying intercepts and slopes model can be written:

\begin{equation}
Y_{ij} = \beta_1+b_{1i}+(\beta_2+b_{2i})x_{ij}+\epsilon_{ij} \quad i=1,...,M, j=1,...,n_i
\end{equation}

with $b_{1i}\sim N(0,\sigma_1^2), b_{2i}\sim N(0,\sigma_2^2)$, and $\epsilon_{ij}\sim N(0,\sigma^2)$.


For the varying intercepts and slopes model we can also say the following. Here, $i$ indexes the subject id, subjects are assumed numbered from 1,\dots,37.

\begin{equation}
\begin{pmatrix}
Y_{i1}\\
Y_{i2}
\end{pmatrix}
\sim
N\left( 
\begin{pmatrix}
\beta_1\\
\beta_2\\
\end{pmatrix}
,
V
\right)
\end{equation}

where 

\begin{equation}
V =
%\begin{pmatrix}
%\sigma_b^2 + \sigma^2 & \sigma_b^2\\
%\sigma_b^2 & \sigma_b^2 + \sigma^2
%\end{pmatrix}=
\begin{pmatrix}
\sigma^2_{b,A} + \sigma^2  &  \rho\sigma_{b,A}\sigma_{b,B}\\
\rho\sigma_{b,A}\sigma_{b,B} & \sigma^2_{b,B}+\sigma^2  \\       
\end{pmatrix}
\end{equation}

Here is a class exercise to get us warmed up. For each of the two models, state 

\begin{enumerate}
\item What the model matrices X and the estimates of $\beta$ are (and what these estimates mean).

Answer:
<<echo=F>>=
head(model.matrix(m0))
head(model.matrix(m1))
## intercept: grand mean, slope: deviation from the grand mean
fixef(m0)
## grand mean:
(gm<-mean(subset(critdata,region=="headnoun")$rt))

## deviations from grand mean:
2*(gm-with(subset(critdata,region=="headnoun"),tapply(rt,IND=list(type2),mean)))
fixef(m1)
@
\item What the estimated variance components of the covariance matrix V is.

Answer:
<<echo=F,eval=F>>=
VarCorr(m1)$subj
## notice that the off-diagonal is:
sqrt(23962)*sqrt(34740)*-1
@
\item Whether all model assumptions are met.

Answer:
<<>>=
library(car)
## residuals assumption not met:
qqPlot(residuals(m0))
qqPlot(residuals(m1))

## random effects (BLUPs') assumptions met:
qqPlot(ranef(m0)$subj)
qqPlot(ranef(m1)$subj[,1])
qqPlot(ranef(m1)$subj[,2])
@

\end{enumerate}

\section{Bayesian data analysis}

Background reading: A major source and reference for these notes is Scott Lynch's book\cite{lynch2007introduction}. I consider this book to be the best book introducing bayesian data analysis out there. However, it does assume that you know at least single variable differential and integral calculus.
Incidentally, a good book that covers all the mathematics you would need to read the Lynch book is available: Gilbert and Jordan.\cite{gilbertjordan}

In my notes I go a bit easier on the reader; it's possible to get by in this course without really knowing calculus (although I will have to show some equations).

If you are reasonably good with calculus and linear algebra and probability theory, read Lynch's book and the references I provide at the end rather than my notes.

The basic idea we will explore in this course is
Bayes' theorem, which allows us to learn from experience and turn a prior distribution into a posterior distribution given data. I stole this neat summary from Lunn et al.\cite{lunn2012bugs}

A central departure from frequentist methodology is that the unknown population parameter is expressed as a random variable. For example, we can talk about how sure we are that the population parameter has a particular value, and this uncertainty is subjective. In the frequentist case, the unknown population parameter is a point value; in the frequentist world, you cannot talk about how sure you are that a parameter has value $\theta$.

%\textbf{Notational conventions}: A binomial distribution, $n$ trials each with probability $\theta$ of occurring, is written $Bi(\theta,n)$. Given a random variable with this distribution, we can write $R\mid \theta, n \sim Bi(\theta,n)$ or $p(r\mid \theta, n) = Bi(\theta,n)$, where $r$ is the realization of $R$. We can drop the conditioning in $R\mid \theta, n$, so that we can write: given $R\sim Bi(\theta,n)$, what is $Pr(\theta_1 < \theta < \theta_2\mid r, n)$.

\section{Steps in bayesian analysis}

\begin{enumerate}
\item Given data, specify a \textbf{likelihood function} or \textbf{sampling density}.
\item Specify \textbf{prior distribution} for model parameters.
\item Derive \textbf{posterior distribution} for parameters given likelihood function and prior density.
\item
Simulate parameters to get \textbf{samples from posterior distribution} of parameters.
\item 
Summarize parameter samples.
\end{enumerate}


\section{Comparison of bayesian with frequentist methodology}

\begin{enumerate}
\item In bayesian data analysis (BDA), the unknown parameter is treated as if it's a random variable, but in reality we are just using a probability density function (pdf) to characterize our uncertainty about parameter values. In frequentist methods, the unknown parameter is a point value. 

\item Frequentists want (and get) $P(data\mid parameters)$, whereas bayesians want (and get) $P(parameters\mid data)$.

What this means is that frequentists will set up a null hypothesis and then compute the probability of the data given the null (the p-value). The bayesian obtains the probability of hypothesis of interest (or the null hypothesis) given the data.


Frequentists will be willing to be wrong about rejecting the null 5\% of the time---under repeated sampling. A problem with that is that a specific data set is a unique thing; it's hard to interpret literally the idea of repeated sampling.

In this context, 
Here's a telling criticism of the frequentist approach by Jeffreys: 
\begin{quote}
``What the use of [the p-value] implies, therefore, is that a hypothesis that may be true may be rejected because it has not predicted observable results that have not occurred.''
\end{quote}
Jeffreys' quote is taken from Lee's book.\cite{lee2012bayesian}

Another interesting quote is from the entsophy blog entry of 23 Sept 2013: ``The question of how often a given situation would arise is utterly irrelevant to the question of how we should reason when it does arise.'' (Attributed to Jaynes.)

\item For frequentists, the probability of an event is defined in terms of (hypothetical) long run relative frequencies; so, one cannot talk about the probability of a single event. For Bayesians, a single event is associated with a probability and this probability is subjective: it depends on the observer's beliefs. An example is the familiar one: the probability of a heads in a coin toss. You believe that any coin you pull out of your pocket is fair; that's a subjective belief. 
From the frequentist viewpoint, the 0.5 probability of a heads is a consequence of long-term relative frequencies under repeated coin tosses.
\end{enumerate}

Bayesians sometimes get pretty worked up about their approach. The goal in this course is not to convert you to a new religion, but rather to learn the methodology. Sometimes bayesian methods make sense, and other times frequentist methods do. 

One argument one could make in favor of using bayesian tools always is that most people who use frequentist methods just don't understand them. Indeed, hardly anyone really understands what a p-value is (many people treat $P(data\mid parameter)$ as if it is identical to $P(parameter\mid data)$), or what a 95\% confidence interval is (many people think a 95\% CI tells you the range over which the parameter is likely to lie with probability 0.95), etc. People abuse frequentist tools (such as publishing null results with low power experiments, not checking model assumptions, chasing after p-values, etc., etc.), and part of the reason for this abuse is that the concepts (e.g., confidence intervals) are very convoluted, or don't even address the research question.
Regarding this last point, consider p-values. They tell you the probability that a null hypothesis is true given the data; p-values doesn't tell you anything exactly about the actual \textbf{research} hypothesis. 

Of course, it remains to be seen whether people would abuse Bayesian methods once they get the hang of it! (I guess one thing will never change: in the hands of an amateur, statistics is a devastating weapon. I should know! I have made horrible mistaks in the past, and probably will in the  future too.)

\section{Statistics is the inverse of probability}

In probability, we are given a probability density or mass function f(x)  (see below), and parameters, and we can deduce the probability.

In statistics, we are given a collection of events, and we want to discover the parameters that produced them (assuming f(x) is the pdf that generated the data). The classical approach is:

\begin{enumerate}
\item Estimate parameters assuming $X\sim f(x)$.
\item Do inference.
\end{enumerate}
 
For example, for reading times, we assume a random variable $X_i$ that comes from a normal distribution $N(0,\sigma^2)$ (the null hypothesis). We compute the most likely parameter value that generated the data, the mean of the random variable,\footnote{This is the maximum likelihood estimate.} and compute the probability of observing a value like the mean or something more extreme given the null hypothesis. I write this statement in short-hand as $P(data\mid parameter)$.

\section{How we will proceed}

We will spend some time reviewing very basic probability theory, and spend some time understanding what a \textbf{probability density/mass function} is. Then we will look at the important concepts of \textbf{joint, marginal, and conditional distributions}; here, you don't need to know how to derive anything, you just need to know what these are. I will show some some derivations, though. Next, we will study \textbf{maximum likelihood}. Then we will get into \textbf{bayesian data analysis}; this will be the more practical part of the course. There will be one theoretical excursion in this practical part; the study of \textbf{Monte Carlo Markov Chain} sampling techniques. But even this will be relatively painless. In the last part of the course we will work through several typical case studies and exercises, using examples from psycholinguistics, usually from my own lab, but we will also use other people's data when it's available.

At the end of the course, I will give some advice on what to read next. There are some really good books on bayesian data analysis, but there are also a lot of very obscure ones that only statistians or mathematicians can follow. Unfortunately, statisticians seem to generally write only for other statisticians; there are very few of them out there who can communicate to the lay world or even the semi-lay world. But there are a few.
