
\chapter{Linear mixed models}

We start with a simple example, the analysis of Gibson and Wu's 2012 dataset on Chinese relative clauses.\cite{gibsonwu} This data will be one of our running examples in this course.

\section{Linear mixed models analysis using lmer}

%to-do:
%\section{Fitting a linear mixed model using Gibbs sampling (varying intercepts)}

<<echo=F,eval=F,include=F>>=
#source: http://people.duke.edu/~neelo003/r/Ranint.r
# Random Intercept Gibbs.r 
# Simulate random intercept model and fit random intercept gibbs
# Assumes BALANCED DESIGN...see random slope.r for unbalanced example
# Dec 14, 2007
##################################

# Required Packages
 library(lme4)    # To test frequentist fit of random intercept model
 library(mvtnorm)    # For rmvnorm funcction

#################
# SIMULATE DATA #
#################

 set.seed(250)
 k<-3					# Number of parms (including intercept)
 n<-100				# sample size
 ni<-10				# no. obs per group
 N<-n%*%ni				# Total number of obs

 # Random Variance Comp
   sigma.e2<-4			# error (within subj) variance
   sigma.b2<-2			# random effect (b/w subj) variance 

 # Fixed Effect Parms
   beta<-c(1,1.5,-1.75) 		# Fixed effect coefficients
   b<-rnorm(n,0,sqrt(sigma.b2))	# Random effects
	
 # Covariates
   x1<-rbinom(N,1,.5)
   x2<-rnorm(N,10,3)
   X<-cbind(rep(1,N),x1,x2)
 
 # Response
   y<-X%*%beta+rep(b,each=ni)+rnorm(N,0,sqrt(sigma.e2)) 
   id<-rep(1:n,each=ni)

 fit<-lmer(y~X-1+(1|id))  	# REML Fit


##########################
# Set Priors, Inits, etc #
##########################

# Priors 
  mu0<-rep(0,k)			# Prior Mean for beta
  t0beta<-diag(.01,k)		# Prior Precision Matrix of beta (vague), independent
  d0<-g0<-l0<-m0<-.001		# Hyperpriors for tau and taub 
 
# Inits
  taub<-tau<-1				# Within-subject and between-subject precisions
  b<-rep(0,n)				# Random effects
  beta<-mbeta<-vbeta<-rep(0,k)	# Posterior mean and var of beta
  Z<-kronecker(diag(n),rep(1,ni)) 	# Random effect design matrix used to update b

# Store Results
  nsim<-5000
  Betas<-matrix(0,nsim,k)	# Fixed Effects
  bs<-matrix(0,nsim,n)		# Random Effect Posterior Predictions
  taus<-taubs<-rep(0,nsim)	# Precision Parms

# Fixed Posterior Hyperparms for tau and taub (thus, not updated in sampler)
  d<-d0+N/2
  l<-l0+n/2

###################
# GIBBS SAMPLER	#
###################
for (i in 1:nsim) {
 # Update Beta 
   vbeta<-solve(t0beta+tau*crossprod(X,X))
   mbeta<-vbeta%*%(t0beta%*%mu0 + tau*crossprod(X,y-Z%*%b))
   Betas[i,]<-beta<-c(rmvnorm(1,mbeta,vbeta))

 # Update b
   vb<-1/(taub+ni*tau)	# Since ni is same for all n
   mb<-vb*(tau*crossprod(Z,y-X%*%beta))
   bs[i,]<-b<-rnorm(n,mb,sqrt(vb))

 # Update tau
    zb<-rep(b,each=ni)	# Z%*%b for random intercept model
    g<-g0+crossprod(y-X%*%beta-zb,y-X%*%beta-zb)/2
    taus[i]<-tau<-rgamma(1,d,g)

 # Update taub 
   m<-c(m0+crossprod(b,b)/2)
   taubs[i]<-taub<-rgamma(1,l,m)
  
#  if (i%%100==0) print(i)
} 

###########
# Results #
###########
 # Posterior Means
  mbeta<-apply(Betas[1501:nsim,],2,mean)
  s2b<-mean(1/taubs[1501:nsim])
  s2e<-mean(1/taus[1501:nsim])
  cat("Posterior Mean of Beta = ",mbeta,"\n")
  cat("Posterior Mean of Sigma.e2 and Simga.b2 = ",c(s2e,s2b)) 

 # Trace Plot
  plot(1501:nsim,Betas[1501:nsim,2], type="l",col="lightgreen")
  abline(h=mean(Betas[1501:nsim,2]),col="blue")
@

%\section{Example 1: using self-paced reading data}

First we load the data and set things up for analysis.

<<label=gwdataload>>=
expt1 <- read.table("data/example1spr.txt")

colnames(expt1) <- c("subj","item","type",
                     "pos","word", "correct","rt")

data<-expt1[ , c(1, 2, 3, 4, 6, 7)]

## rename data frame
dat<-data

#head(dat)

## 37 subjects
#length(unique(dat$subj))
#xtabs(~subj+item,dat)
#length(sort(unique(dat$subj)))
@

Next we do some pre-processing to get ready for analysis:

<<label=gwpreproc,echo=FALSE>>=
questions <- subset(expt1,correct%in%c(1,0)) 
questions$correct <- as.numeric(as.character(questions$correct))

#subj-ext: c0 | c1 | c2 | c3 | say | V1 | N1 | DE | N2 | N2+1 | N2+2 | N2+3
#obj-ext : c0 | c1 | c2 | c3 | say | N1 | V1 | DE | N2 | N2+1 | N2+2 | N2+3

# rc started at pos 5; de was always at pos7

#       subj-ext  obj-ext   
#so          -1        1   effect of subject-object RC

de2<-cbind(subset(expt1,pos=="5"),region="de2") ## position de-2
de1<-cbind(subset(expt1,pos=="6"),region="de1") ## position de-1

## de2 and de1 are always contiguous, on successive lines. No missing data:
#dim(de2)
#dim(de1)

## sum up V+N and N+V sequences:
de12sum<-de1$rt+de2$rt

## add to de1 a column for the sum of de1 and d2 rt's.
## Note that this is a destructive step, the rt column is
## overwritten. I had to do this to be able to assemble the whole
## dataset into critdata below. Everything is recoverable by 
## rerunning the script.
de1$rt<-de12sum
de <- cbind(subset(expt1,pos=="7"),region="de")
hnoun <- cbind(subset(expt1,pos=="8"),region="headnoun")
## sum up de and noun RTs:
de.hnoun.rt<-de$rt+hnoun$rt
## make a copy of hnoun:
dehnoun<-hnoun
## replace rt column in hnoun with de.hnoun.rt:
dehnoun$rt<-de.hnoun.rt
## rename region
dehnoun$region<-"dehnoun"
hnoun1 <- cbind(subset(expt1,pos=="9"),region="headnoun1")

critdata <- rbind(de1,de,dehnoun,hnoun,hnoun1)

type2<-factor(ifelse(critdata$type=="obj-ext","object relative","subject relative"))
critdata$type2<-type2
@

We can look at the distributions of reading time in each region of interest:

<<>>=
library(lattice)
bwplot(rt~type2,subset(critdata,region=="headnoun"),
       xlab="relative clause type",ylab="reading time (ms)")

bwplot(log(rt)~type2,subset(critdata,region=="headnoun"),
       xlab="relative clause type",ylab="log reading time (log ms)")

bwplot(-1000/rt~type2,subset(critdata,region=="headnoun"),
       xlab="relative clause type",ylab="negative reciprocal reading time (-1/sec)")
@

Set up the contrast coding:

<<>>=
library(lme4)
library(car)
library(ggplot2)

## treatment contrasts should be OK for analysis...
#contrasts(critdata$type)

## but let's do a regular anova style centering, sum contrasts:
critdata$so<- ifelse(critdata$type%in%c("subj-ext"),-0.5,0.5)
@


The Box-Cox transform suggests using the inverse for the head noun and the region after:

<<gwboxcox>>=
par( mfrow=c(2,2) )

library(MASS)
boxcox(rt~type*subj,
       data=critdata[critdata$region=="de1", ])

boxcox(rt~type*subj,
       data=critdata[critdata$region=="de", ])

boxcox(rt~type*subj,
       data=critdata[critdata$region=="headnoun", ])

boxcox(rt~type*subj,
       data=critdata[critdata$region=="headnoun1", ])

## transform:
critdata$rrt <- -1000/critdata$rt

means.rrt<-round(with(critdata,
                      tapply(rrt,IND=list(region,type),mean)),
                 digits=3)

means.rt<-round(with(critdata,
                     tapply(rt,IND=list(region,type),mean)),
                digits=0)

library(xtable)
#xtable(cbind(means.rt,means.rrt))
@

\begin{marginfigure}
<<fig=TRUE,echo=F>>=
<<gwboxcox>>
@
\caption{The results of the Box-Cox procedure.}\label{fig:gwboxcox}
\end{marginfigure}

<<gwboxcox2>>=
boxplot(rrt~type,subset(critdata,region=="de1")) 
boxplot(rrt~type,subset(critdata,region=="de")) 

boxplot(rrt~type,subset(critdata,region=="dehnoun"))
boxplot(rrt~type,subset(critdata,region=="headnoun")) 
boxplot(rrt~type,subset(critdata,region=="headnoun1")) 
@


We have predictions for the head noun and the word after that, but with a variance stabilizing transform (reciprocal RT) these are not borne out, cf.\ the published paper's results based on raw RTs (also see below). For more detail see Vasishth et al 2013.\cite{VasishthetalPLoSOne2013}

<<label=gwlmerfitsrrt>>=
## no effect at headnoun. 
## cf. spurious effect at head noun in raw rts:
gw.hn.rrt <- lmer(rrt~so+(1|item)+(1+so|subj),
                  subset(critdata,region=="headnoun"))

summary(gw.hn.rrt)
headnoun.dat<-subset(critdata,region=="headnoun")

gw.hn.rt <- lmer(rt~so+(1|item)+(1+so|subj),
                  headnoun.dat)

gw.hn.lrt <- lmer(log(rt)~so+(1|item)+(1+so|subj),
                  headnoun.dat)

#summary(gw.hn.rt)

gw.hn.rt.trimmed <- lmer(rt~so+(1|item)+(1+so|subj),
                  headnoun.dat[abs(scale(residuals(gw.hn.rt)))<2.5,]
)

#summary(gw.hn.rt.trimmed)

## check how many data points we have by subject and item:
#xtabs(~subj+so,subset(critdata,region=="headnoun"))
#xtabs(~item+so,subset(critdata,region=="headnoun"))

## contrast coding:
#critdata$so<- ifelse(critdata$type%in%c("subj-ext"),-0.5,0.5)

gw.hn.rt <- lmer(rt~so+(1+so|item)+(1+so|subj),
                 subset(critdata,region=="headnoun"))

gw.hn.logrt <- lmer(log(rt)~so+(1+so|item)+(1+so|subj),
                    subset(critdata,region=="headnoun"))

gw.hn.rrt <- lmer(-1000/rt~so+(1+so|item)+(1+so|subj),
                  subset(critdata,region=="headnoun"))
@

Classical aggregated analysis assuming non-equal variance:
<<>>=
hnoun<-subset(critdata,region=="headnoun")

## classical aggregrated analysis:
hnoun.subj.means<-aggregate(rt~subj+type,
                            mean,data=hnoun)
t.test(subset(hnoun.subj.means,type=="subj-ext")$rt,
       subset(hnoun.subj.means,type=="obj-ext")$rt,paired=T,var.equal=FALSE)
@

@

<<label=gwlmerfitsrrtresid>>=
## examine residuals:
library(car)
op<-par(mfrow=c(1,3),pty="s")
qqPlot(residuals(gw.hn.rt),
       ylab="residuals of raw reading times")
qqPlot(residuals(gw.hn.logrt),
       ylab="residuals of log reading times")

qqPlot(residuals(gw.hn.rrt),
       ylab="residuals of negative reciprocal reading times")
@

\begin{marginfigure}
<<fig=T,echo=F>>=
<<gwlmerfitsrrtresid>>
@
\caption{Residuals in lmer models with raw RTs, log RTs, and negative reciprocals.}\label{fig:threeresids}
\end{marginfigure}

On this raw reading time scale, the differences in rt are about 178 ms at the head noun (OR advantage):

<<label=gwmeans>>=
means<-with(critdata,tapply(rt,IND=list(region,type),mean))
@

However, standard deviation is not comparable:

<<>>=
sds<-with(critdata,tapply(rt,IND=list(region,type),sd))
@

At the head noun, the ratio of variances is:

<<label=gwsdheadnoun>>=
round(sds[4,2]/sds[4,1],digits=1)
@

Note that Gibson and Wu fit raw reading times, and got significant effects (OR advantage). Here is an lmer fit analogous (but not identical) to what they did:

<<gwlmerrawrtfits>>=
##head noun:
gw.hn <- lmer(rt~so+(1|item)+(1|subj),subset(critdata,region=="headnoun"))
@

The model estimates that ORs are about 120 ms easier to process than SRs at the head noun.

However, statistical significance here is a consequence of the normality assumption (of residuals) not being satisfied; I think that, more precisely, it's the equal variance assumption that's an issue (SR variance is much higher due to those extreme values). 

<<gwlmerrawrtresid>>=
qqPlot(residuals(gw.hn))
@

\begin{marginfigure}
<<fig=TRUE,echo=F>>=
<<gwlmerrawrtresid>>
@
\caption{Residuals in the model using raw reading times.}\label{fig:gwlmerrawrtresid}
\end{marginfigure}

Compare this with the reciprocal rt's residuals.

<<gwlmerreciprocalrtresid,echo=F>>=
qqPlot(residuals(gw.hn.rrt))
@

%\begin{marginfigure}
<<fig=FALSE,echo=F>>=
<<gwlmerreciprocalrtresid>>
@
%\caption{Residuals of negative reciprocals.}\label{fig:negrec}
%\end{marginfigure}

In the remaining sections, I will fit the three major types of model used in psycholinguistics (a) crossed varying intercepts by subject and by item, (b) varying intercepts and slopes by subject and varying intercepts by item, without a correlation estimated, (c) and varying intercepts and slopes by subject and varying intercepts by item, with correlation estimated.

For more complex models, which will come up in later case studies, we will switch to Stan.

\section{Bayesian analysis: Crossed varying intercepts for subject and item}

We will use the lmer fit to decide on initial values when we do Gibbs sampling.

<<>>=
## using linear mixed model to figure out init values:
m1<-lmer(rrt~so+(1|subj)+(1|item),subset(critdata,region=="headnoun"))

## estimated sd of varying intercept:
sigma.u<-attr(VarCorr(m1)$subj,"stddev")
sigma.w<-attr(VarCorr(m1)$item,"stddev")

## estimated residual sd:
sigma.e<-attr(VarCorr(m1),"sc")

beta<-fixef(m1)

headnoun<-subset(critdata,region=="headnoun")
@

Set up data for JAGS. \textbf{Note that the subject and item should be relabeled as integers in increasing order. This is important for the JAGS model.}

<<>>=
headnoun.dat <- list( subj = sort(as.integer( factor(headnoun$subj) )),
                      item = sort(as.integer( factor(headnoun$item) )),
                   rrt = headnoun$rrt,
                    so = headnoun$so,
                    N = nrow(headnoun),
                    I = length( unique(headnoun$subj) ),
                    K = length( unique(headnoun$item) )  
                      )
@

Set up four chains:

<<>>=
headnoun.ini <- list( list( sigma.e = sigma.e/3,
                         sigma.u = sigma.u/3,
                         sigma.w = sigma.w/3,    
                            beta = beta   /3 ),
                   list( sigma.e = sigma.e*3,
                         sigma.u = sigma.u*3,
                         sigma.w = sigma.w*3,
                            beta = beta   *3 ),
                   list( sigma.e = sigma.e/3,
                         sigma.u = sigma.u*3,
                         sigma.w = sigma.w*3,
                            beta = beta   /3 ),
                   list( sigma.e = sigma.e*3,
                         sigma.u = sigma.u/3,
                         sigma.w = sigma.w/3,
                            beta = beta   *3 ) )

@


The JAGS model below assumes that we have reciprocal rts. 

<<label=gwjagsmodelcrossedrandomintercepts>>=
cat("
# Fixing data to be used in model definition
model
   {
   # The model for each observational unit 
  #   (each row is a subject's data point)
     for( j in 1:N )
     {
     mu[j] <- beta[1] + beta[2] * ( so[j] )  + u[subj[j]] + w[item[j]]
     rrt[j] ~ dnorm( mu[j], tau.e )
      }

   # Random effects for each subject
     for( i in 1:I )
     {
     u[i] ~ dnorm(0,tau.u)
     }

     # Random effects for each item
     for( k in 1:K )
     {
     w[k] ~ dnorm(0,tau.w)
     }
  
   # Uninformative priors:
     
   # Fixed effect intercept and slope
     beta[1] ~ dnorm(0.0,1.0E-5)
     beta[2] ~ dnorm(0.0,1.0E-5)

   # Residual (within-person) variance
     tau.e <- pow(sigma.e,-2)
     sigma.e  ~ dunif(0,10)

   # Between-person variation
     tau.u <- pow(sigma.u,-2)
     sigma.u  ~ dunif(0,10)   

     # Between-item variation
     tau.w <- pow(sigma.w,-2)
     sigma.w  ~ dunif(0,10) 
    ## I get normal residuals only if I assume a N(0,100) prior 
    ## or one with larger variance
        
   }",
     file="JAGSmodels/gwheadnouncrossedrandom.jag" )
@

Define which variables you want to track:

<<>>=
track.variables<-c("beta","sigma.e","sigma.u","sigma.w")
@

Run model:

<<>>=
library(rjags)
headnoun.mod <- jags.model( file = "JAGSmodels/gwheadnouncrossedrandom.jag",
                         data = headnoun.dat,
                     n.chains = 4,
                        inits = headnoun.ini,
                      n.adapt =2000 , quiet=T)

@

Generate posterior samples:

<<>>=
headnoun.res <- coda.samples( headnoun.mod,
                                 var = track.variables,
                              n.iter = 10000,
                                thin = 20 )
@

One way to assess convergence is with 
the Gelman-Rubin (or Brooks-Gelman-Rubin) diagnostic. This involves sampling from multiple chains and then comparing between and within group variability. It's analogous to the F-score in anova.
Within variance is represented by the mean width of the 95\% posterior Credible Intervals (CrI) of all chains, from the final T iterations.  
Between variance is represented by the width of the 95\% CrI using all chains pooled together (for the T iterations). If the ratio $\hat{R}=B/W$ is approximately 1, we have convergence.

<<>>=
## Gelman-Rubin convergence diagnostic:
gelman.diag(headnoun.res)
@

And finally, plot results:

<<headnounsres>>=
plot(headnoun.res)
@

\begin{marginfigure}
<<fig=TRUE>>=
<<headnounsres>>
@
\caption{Posterior distribution for reading times at head noun.}\label{fig:headnounres}
\end{marginfigure}

\section{Bayesian analysis: Varying intercepts and slopes by subject, varying intercepts by item, no correlation estimated}
\label{nocorrelationexamples}

We now fit a crossed varying intercepts model (items and subject) with varying slopes for subjects. We don't estimate the correlation between intercepts and slopes; I show two ways to fit this model in JAGS.

We use sum contrast coding as above.

First we define the model. 
Let $i$ range over subjects ($i=1,\dots,n$); $j$ ranges over the two conditions ($j=1,2$); and $k$ range over the items ($k=1,\dots,m$).

Specify JAGS model with varying intercepts for subjects ($u_{0i}$) and items ($w_{0k}$), and varying slopes ($u_{1i}$) by subject.

\begin{equation}
y_{ijk} = (\beta_{0}+u_{0i} + w_{0k}) + (\beta_1 + u_{1i}) + e_{ijk}
\end{equation}

\noindent
where 

\begin{equation}
\begin{pmatrix}
  u_{0i} \\ 
  u_{1i}
\end{pmatrix}
\sim 
N(
\begin{pmatrix}
  0 \\ 
  0
\end{pmatrix},
\begin{pmatrix}
  \sigma_{u_{0i}}^2  & \rho\sigma_{u_{0i}} \sigma_{u_{1i}}\\ 
  \rho\sigma_{u_{0i}} \sigma_{u_{1i}} &\sigma_{{u_{1i}}}^2
\end{pmatrix}
)\quad \hbox{ and } w_{0k} \sim N(0, \sigma_{w0k})  
\end{equation}

and 

\begin{equation}
\epsilon_{ijk} \sim N(0,\sigma^2)
\end{equation}

In other words, the variance covariance matrix for the varying intercepts and slopes by subject is:

\begin{equation}
\Sigma = 
\begin{pmatrix}
  \sigma_{u_{0i}}^2  & \rho\sigma_{u_{0i}} \sigma_{u_{1i}}\\ 
  \rho\sigma_{u_{0i}} \sigma_{u_{1i}} &\sigma_{{u_{1i}}}^2
\end{pmatrix}
\end{equation}

\noindent
with $\rho$ not estimated.

We are going to derive this $\Sigma$ in JAGS. We do this by modeling the precision matrix $\Omega$, which is the inverse of the variance-covariance matrix $\Sigma$, as a Wishart distribution (which takes a positive definite $p\times p$ matrix $R$, and $k\geq p$, as parameters; see JAGS manual). The Wishart distribution is the multivariate generalization of the gamma distribution. Recall that the inverse gamma is a conjugate prior density for the variance in a univariate normal model; the inverse Wishart is a conjugate prior density for variance in the multivariate case above. So, we can model a precision matrix using the Wishart, and the variance-covariance matrix using the inverse Wishart.  By taking the inverse of $\Omega$ we get $\Sigma$.
%%to-do: need more detail on Wishart.


Here is the model:

<<label="jagsmodelintslopeuninf",echo=T>>=
cat("
# Fixing data to be used in model definition
data
   {
   zero[1] <- 0
   zero[2] <- 0
## some prior guess for precision matrix:
   R[1,1] <- 0.1
   R[1,2] <- 0
   R[2,1] <- 0  
   R[2,2] <- 0.5
   }
# Then define model
model
   {
   # Intercept and slope for each person, including random effects
     for( i in 1:I )
     {
     u[i,1:2] ~ dmnorm(zero,Omega.u)
     }
    
         # Random effects for each item
     for( k in 1:K )
     {
     w[k] ~ dnorm(0,tau.w)
     }

   # Define model for each observational unit
     for( j in 1:N )
     {
     mu[j] <- ( beta[1] + u[subj[j],1] ) +
              ( beta[2] + u[subj[j],2] ) * ( so[j] ) + w[item[j]]
     rrt[j] ~ dnorm( mu[j], tau.e )
     }

   #------------------------------------------------------------
   # Priors:

   # Fixed intercept and slope (uninformative)
     beta[1] ~ dnorm(0.0,1.0E-5)
     beta[2] ~ dnorm(0.0,1.0E-5)
          
   # Residual variance
     tau.e <- pow(sigma.e,-2)
   sigma.e  ~ dunif(0,100)

   # Define prior for the variance-covariance matrix of the random effects for subjects
     Sigma.u <- inverse(Omega.u)
     Omega.u  ~ dwish( R, 2 )
    
    # Between-item variation
     tau.w <- pow(sigma.w,-2)
     sigma.w  ~ dunif(0,10)            
   }",
     file="JAGSmodels/gwintslopeuninf.jag" )
@

We re-do the data set-up just for completeness:

<<label="formatdata">>=
## 
#headnoun<-subset(data,region=="headnoun")
#headnoun$region<-factor(headnoun$region)

headnoun.dat <- list(subj = 
              sort(as.integer(factor(headnoun$subj))),
                      item = 
              sort(as.integer(factor(headnoun$item))),
                    rrt = headnoun$rrt,
                    so = headnoun$so,
                    N = nrow(headnoun),
                    I = length(unique(headnoun$subj)),
                    K = length(unique(headnoun$item)))
@

Set up four chains (initial values derived from lmer model):

<<label="setupchainsheadnoun">>=

m2<-lmer(rrt~so+(1|subj)+(0+so|subj)+(1|item),headnoun)

( sigma.e <- attr(VarCorr(m2),"sc") )
## Build precision matrix, inverse of VarCorr mat:
Sigma<-matrix(rep(NA,4),ncol=2)
Sigma[1,1]<-attr(VarCorr(m2)$subj.1,"stddev")
Sigma[2,2]<-attr(VarCorr(m2)$subj,"stddev")
## uncorrelated standard errors of variance components:
Sigma[1,2]<-Sigma[2,1]<-0
( Omega.u <- solve( Sigma ))
(sigma.w<-attr(VarCorr(m2)$item,"stddev"))
( beta    <- fixef( m2 ) )
@

Set up initial values:

<<>>=
headnoun.ini <- list( list( sigma.e = sigma.e/3,
                         Omega.u = Omega.u/3,
                            sigma.w = sigma.w/3,
                            beta = beta   /3 ),
                   list( sigma.e = sigma.e*3,
                         Omega.u = Omega.u*3,
                         sigma.w = sigma.w*3,
                            beta = beta   *3 ),
                   list( sigma.e = sigma.e/3,
                         Omega.u = Omega.u*3,
                         sigma.w = sigma.w/3,
                            beta = beta   /3 ),
                   list( sigma.e = sigma.e*3,
                         Omega.u = Omega.u/3,
                         sigma.w = sigma.w*3,
                            beta = beta   *3 ) )
@

Set up JAGS model:

<<label="runjagsmodelheadnoun">>=
library(rjags)
headnoun.mod <- jags.model(
                file = "JAGSmodels/gwintslopeuninf.jag",
                data = headnoun.dat,
                n.chains = 4,
                inits = headnoun.ini,
                n.adapt = 5000 ,quiet=T)            
@


Define variables to track, and sample from posterior:

<<>>=
track.variables<-c("beta","sigma.e","Sigma.u","sigma.w")

headnoun.res <- coda.samples( headnoun.mod,
                                 var = track.variables,
                              n.iter = 10000,
                                thin = 20 )
@

Then, summarize results, check convergence, and plot (not shown).

<<eval=F,echo=T>>=
summary( headnoun.res )
## compare with lmer:
# Groups   Name        Std.Dev.
# subj     so          0.228   
# subj.1   (Intercept) 0.609 <- a bit larger in lmer  
# item     (Intercept) 0.331   
# Residual             0.946   
#                Mean     SD Naive SE Time-series SE
## Note that Sigma contains variances
## but lmer prints out sds:
#Sigma.u[1,1]  0.2555 0.0934 0.002088       0.002120
#Sigma.u[2,1] -0.0251 0.0584 0.001306       0.001349
#Sigma.u[1,2] -0.0251 0.0584 0.001306       0.001349
#Sigma.u[2,2]  0.1171 0.0544 0.001216       0.001156
#sigma.e       0.9585  0.9958  1.0159  1.03892  1.0831
#sigma.w       0.3100 0.1499 0.003351       0.004996
## multivariate=F: see help.
gelman.diag(headnoun.res,multivariate=F)

par( mfrow=c(3,3) )
plot(headnoun.res)
@

Here is an alternative way to fit this model, which does not estimate the correlation between intercepts and slopes.
We could have defined the model without a prior for the precision matrix, because the two variance components by subject (varying intercepts and varying slopes) are independent:

<<>>=
cat("
model
   {
   # Intercept and slope for each person, including random effects
     for( i in 1:I )
     {
     u[i,1] ~ dnorm(0,tau.u.int)
     u[i,2] ~ dnorm(0,tau.u.slopes)
     }
    
         # Random effects for each item
     for( k in 1:K )
     {
     w[k] ~ dnorm(0,tau.w)
     }

   # Define model for each observational unit
     for( j in 1:N )
     {
     mu[j] <- ( beta[1] + u[subj[j],1] ) +
              ( beta[2] + u[subj[j],2] ) * ( so[j] ) + w[item[j]]
     rrt[j] ~ dnorm( mu[j], tau.e )
     }

   #------------------------------------------------------------
   # Priors:

   # Fixed intercept and slope (uninformative)
     beta[1] ~ dnorm(0.0,1.0E-5)
     beta[2] ~ dnorm(0.0,1.0E-5)
          
   # Residual variance
     tau.e <- pow(sigma.e,-2)
   sigma.e  ~ dunif(0,100)
    
    # Between-subj variation
    tau.u.int <- pow(sigma.u.int,-2)
    sigma.u.int ~ dunif(0,10)
    tau.u.slopes <- pow(sigma.u.slopes,-2)
    sigma.u.slopes ~ dunif(0,10)

    # Between-item variation
     tau.w <- pow(sigma.w,-2)
     sigma.w  ~ dunif(0,10)            
   }",
     file="JAGSmodels/gwintslopeuninfv2.jag" )
@

We need to work out the initial values again because we have to set them up differently now (the precision matrix is no longer in the picture):

<<>>=
## the other initial values remain unchanged.
## now we have two independent std devs for int and slope:
(sigma.u.int <-attr(VarCorr(m2)$subj.1,"stddev"))
(sigma.u.slopes <-attr(VarCorr(m2)$subj,"stddev"))
@

Set up initial values:

<<>>=
headnoun.ini <- list( list( sigma.e = sigma.e/3,
                         sigma.u.int = sigma.u.int/3,
                         sigma.u.slopes = sigma.u.slopes/3,
                            sigma.w = sigma.w/3,
                            beta = beta   /3 ),
                   list( sigma.e = sigma.e*3,
                         sigma.u.int = sigma.u.int*3,
                         sigma.u.slopes = sigma.u.slopes*3,                         
                         sigma.w = sigma.w*3,
                            beta = beta   *3 ),
                   list( sigma.e = sigma.e/3,
                         sigma.u.int = sigma.u.int*3,
                         sigma.u.slopes = sigma.u.slopes*3,
                         sigma.w = sigma.w/3,
                            beta = beta   /3 ),
                   list( sigma.e = sigma.e*3,
                         sigma.u.int = sigma.u.int/3,
                         sigma.u.slopes = sigma.u.slopes/3,
                         sigma.w = sigma.w*3,
                            beta = beta   *3 ) )
@

Set up JAGS model:

<<label="runjagsmodelheadnoun">>=
library(rjags)
headnoun.mod <- jags.model(
                file = "JAGSmodels/gwintslopeuninfv2.jag",
                data = headnoun.dat,
                n.chains = 4,
                inits = headnoun.ini,
                n.adapt = 5000 ,quiet=T)            
@


Define variables to track, and sample from posterior:

<<>>=
track.variables<-c("beta","sigma.e","sigma.u.int","sigma.u.slopes","sigma.w")

headnoun.res <- coda.samples( headnoun.mod,
                                 var = track.variables,
                              n.iter = 10000,
                                thin = 20 )
@

Then, summarize results, check convergence, and plot (not shown).

<<eval=F,echo=T>>=
summary( headnoun.res )
## compare with lmer:
# Groups   Name        Std.Dev.
# subj     so          0.228   
# subj.1   (Intercept) 0.609 <- a bit larger in lmer  
# item     (Intercept) 0.331   
# Residual             0.946   
#                Mean     SD Naive SE Time-series SE
#Sigma.u[1,1]  0.2555 0.0934 0.002088       0.002120
#Sigma.u[2,1] -0.0251 0.0584 0.001306       0.001349
#Sigma.u[1,2] -0.0251 0.0584 0.001306       0.001349
#Sigma.u[2,2]  0.1171 0.0544 0.001216       0.001156
#sigma.e       0.9585  0.9958  1.0159  1.03892  1.0831
#sigma.w       0.3100 0.1499 0.003351       0.004996
## V2, without precision matrix prior:
#                  Mean     SD Naive SE Time-series SE
#sigma.e         1.0187 0.0321 0.000717       0.000758
## note that these are standard deviations:
## cf Sigma[1,1] and Sigma[2,2] above, which are
## variances!
#sigma.u.int     0.5360 0.0980 0.002190       0.002503
#sigma.u.slopes  0.1549 0.1055 0.002358       0.005612
#sigma.w         0.2766 0.1487 0.003326       0.005627
@

We get very similar results using both methods.

\subsection{The correlation between varying intercepts and slopes}

In the above model, we do not estimate $\rho$. Recall that the correlation is that between the varying intercepts and slopes (the BLUPs). Even though $\rho$ is not estimated in the model, it can be calculated, as I discuss below.

First, a refresher: If we were interesting in estimating the relationship between the different intercepts and slopes for each subject for the factor so, we could fit a separate linear model for each subject, compute the intercept and slope for each subject, and just plot the relationship. We can also compute the correlation, see below.

\begin{marginfigure}
<<fig=T>>=
intslopes<-lmList(rrt~so|subj,subset(critdata,region=="headnoun"))

intercepts<-coef(intslopes)[,1]
slopes<-coef(intslopes)[,2]
plot(intercepts~slopes,xlab="slopes",ylab="intercepts")
cor(intercepts,slopes)
@
\caption{Intercepts and slopes by subject.}\label{fig:intslopeslmlist}
\end{marginfigure}

In linear mixed models, we don't compute these intercepts and slopes separately for each subject, but rather we ``borrow strength from the grand mean'' and conservatively cause the intercepts and slopes to gravitate towards the grand mean intercept and slope. Gelman and Hill present a nice discussion of this kind of ``shrinkage'' (but everyone who took my summer course should have seen this before).

To estimate the above correlation from our model, we just need to figure out what the estimated value is from the model of the off-diagonal in $\Sigma$. The summary output of the model shows all the components of $\Sigma$. Then we can use the fact that the off-diagonal value is the covariance, i.e., $Covariance = \rho \sigma_A \sigma_B$, where $\sigma_A$ is the standard deviation of the intercepts (this can be gotten from the posterior estimate for $\Sigma$, from the [1,1] cell), and $\sigma_B$ is the standard deviation of the slopes. \textbf{Do this calculation now for this data}.

The point here is that even though we did not estimate the correlation in the model, we can calculate it. In other words, the difference between the two specifications we use in psycholinguistics with lmer:

\begin{verbatim}
(1|subj) + (0+so|subj)      and       (1+so|subj)
\end{verbatim}	

is only that in the first case no correlation is printed out, and in the second it is. Note that the value get in the second case is not totally identical to the one computed by hand in the first case, but it's quite similar.
%%to-do: need to check this.

\begin{Homework} \label{varyingintslope}
Fit a varying intercepts and slopes model (no correlation estimated) for the region following the head noun.
%Redo the above model with varying intercepts and slopes by condition for subject. 
%Try the same with items (this will probably cause a failure---not enough data; note that lmer will silently succeed in this case, but JAGS will probably crash, I will discuss this in class).
\end{Homework}


\begin{Homework}\label{lexdecredux}
Return to the lexdec dataset from the regression chapter, and fit a model with varying intercepts for subject, and varying intercepts for Word (i.e., subject and Word are random effects).  
\end{Homework}


\section{Bayesian analysis: Fitting a varying intercepts, varying slopes model for subjects, including correlation term}

The model will be:

<<>>=
m3 <- lmer(rrt~so+(1+so|subj)+(1|item),
          subset(critdata,region=="headnoun"))
@

Here is the JAGS code:

<<echo=T>>=
cat("
# Fixing data to be used in model definition
data
{
    zero[1] <- 0
    zero[2] <- 0
## the var-cov matrix is defined below
}
    # Then define model
    model
{
    # Intercept and slope for each person, including random effects
    for( i in 1:I )
{
    u[i,1:2] ~ dmnorm(zero,Omega.u)
}
    
    # Random effects for each item
    for( k in 1:K )
{
    w[k] ~ dnorm(0,tau.w)
}
    
    # Define model for each observational unit
    for( j in 1:N )
{
    mu[j] <- ( beta[1] + u[subj[j],1] ) +
    ( beta[2] + u[subj[j],2] ) * ( so[j] ) + w[item[j]]
    rrt[j] ~ dnorm( mu[j], tau.e )
}
    
    #------------------------------------------------------------
    # Priors:
    
    # Fixed intercept and slope (uninformative)
    beta[1] ~ dnorm(0.0,1.0E-5)
    beta[2] ~ dnorm(0.0,1.0E-5)
    
    # Residual variance
    tau.e <- pow(sigma.e,-2)
    sigma.e  ~ dunif(0,100)
    
    # Define prior for the variance-covariance matrix of the random effects for subjects
    ## precision:
    Omega.u  ~ dwish( R, 2 )
    ## R matrix:
    R[1,1] <- pow(sigma.a,2)
    R[2,2] <- pow(sigma.b,2)
    R[1,2] <- rho*sigma.a*sigma.b
    R[2,1] <- R[1,2]
    ## Vcov matrix:
    Sigma.u <- inverse(Omega.u)
    ## priors for var int. var slopes
    sigma.a ~ dunif(0,10)
    sigma.b ~ dunif(0,10)
    
    ## prior for correlation:
    rho ~ dunif(-1,1)

    # Between-item variation
    tau.w <- pow(sigma.w,-2)
    sigma.w  ~ dunif(0,10)   
}",
     file="JAGSmodels/gwintslopeuninfcorr.jag" )
@


<<echo=F>>=
#headnoun<-subset(data,region=="headnoun")

headnoun.dat <- list(subj=
                       sort(as.integer( 
                         factor(headnoun$subj) )),
                     item=
                       sort(as.integer( 
                         factor(headnoun$item) )),
                     rrt = headnoun$rrt,
                     so = headnoun$so,
                     N = nrow(headnoun),
                     I = 
                       length( unique(headnoun$subj) ),
                     K = 
                       length( unique(headnoun$item) )  
)

library(rjags)
 
track.variables<-c("beta","sigma.e","sigma.w","rho","sigma.a","sigma.b")

#                   "smallest","mn","largest")

headnoun.mod <- jags.model( file = "JAGSmodels/gwintslopeuninfcorr.jag",
                         data = headnoun.dat,
                     n.chains = 4,
                      n.adapt =2000 , quiet=T)


headnoun.res <- coda.samples( headnoun.mod,
                              var = track.variables,
                              n.iter = 10000,
                              thin = 20 )

summary(headnoun.res)
#plot(headnoun.res)
mcmcChain<-as.matrix(headnoun.res)
@

<<fig=T>>=
## for later comparison:
hist(mcmcChain[,3])
@


\section{Summary}

We can now fit the three classical types of psycholinguistic models in JAGS:

\begin{enumerate}
\item Varying intercepts:
<<>>=
m1<-lmer(rrt~so+(1|subj)+(1|item),headnoun)
@
\item  Varying intercepts and slopes, no correlation estimated:
<<>>=
m1<-lmer(rrt~so+(1|subj)+(0+so|subj)+(1|item),
         headnoun)
@
\item
Varying intercepts and slopes, correlation estimated:
<<>>=
m2<-lmer(rrt~so+(1|subj)+(0+so|subj)+(1|item),
         headnoun)
@
\end{enumerate}

Gelman and Hill 2007 (p.\ 375) say that the second type is inappropriate. But we will often have to fit them because we get a convergence error in the third type of model (not enough data).  I present one solution to this issue next.

\section{What to do about $\pm 1$ correlation estimates}

We often find ourselves with a varying intercepts, varying slopes model where we ask lmer to estimate the correlation and it returns a $+ 1$ or $- 1$ correlation.  Normally, we just back off to not estimating the correlation.
Chung et al have an unpublished paper where they solve this problem in the bayesian setting:

``The key problem solved by our method is the tendency of maximum likelihood
estimates of $\Sigma$ to be degenerate, that is, on the border of positive-definiteness, which
corresponds to \textbf{zero variance or perfect correlation} among some linear combinations of
the parameters. \dots When the maximum likelihood estimate of a hierarchical covariance
matrix is degenerate, this arises from a likelihood that is nearly flat in the relevant
dimension and \textbf{just happens to have a maximum at the boundary}.''
%% to-do: develop a demo

Their recommendation:

``A small amount
of regularization puts the posterior mode inside the allowable space and reduces mean
squared error.
Our solution is a class of weakly informative prior densities for $\Sigma$ that go to zero
on the boundary as $\Sigma$  becomes degenerate, thus ensuring that the posterior mode (i.e.,
the maximum penalized likelihood estimate) is always nondegenerate.''

``We recommend a
class of Wishart priors with a default choice of hyperparameters: the degrees of freedom
is the dimension of $b_j$ plus two and the scale matrix is the identity matrix multiplied
by a large enough number. 
This prior can be expressed as 
\dots a product of gamma(1.5, $\theta$) priors on variances of
the varying effects with rate parameter $\theta\rightarrow 0$ and a function of the correlations (a beta
prior the two-dimensional case).''

Later on they say:

``independent gamma(1.5,$\theta$) priors on both $\sigma_a$ and $\sigma_b$, and a beta(1.5,1.5) prior on ($\rho$ + 1)/2.''


This means that if we have a $\sigma_a$, $\sigma_b$ as varying intercept and varying slope correlations $\rho$:

\begin{verbatim}
## varying intercepts' variance:
sigma_a ~ dgamma(1.5,10^(-4))
## varying slopes' variance:
sigma_b ~ dgamma(1.5,10^(-4))
## correlation:
rho <- rho2*2-1
rho2 ~ dbeta(1.5,1.5)
\end{verbatim}
%%to-do: test this out

Here is what these priors look like.

\begin{marginfigure}
<<fig=T,echo=F>>=
op<-par(mfrow=c(1,2),pty="s")

x<-seq(0,100,by=0.01)
plot(x,dgamma(x,1.5,10^(-4)),type="l",main="Prior for variances")

x<-seq(0,1,by=0.01)
plot(x,dbeta(x,2,2),type="l",main="Prior for correlation")
@
\end{marginfigure}

And here is a JAGS model with the correlation prior shown above:

<<echo=T>>=
cat("
# Fixing data to be used in model definition
data
{
    zero[1] <- 0
    zero[2] <- 0
## the var-cov matrix is defined below
}
    # Then define model
    model
{
    # Intercept and slope for each person, including random effects
    for( i in 1:I )
{
    u[i,1:2] ~ dmnorm(zero,Omega.u)
}
    
    # Random effects for each item
    for( k in 1:K )
{
    w[k] ~ dnorm(0,tau.w)
}
    
    # Define model for each observational unit
    for( j in 1:N )
{
    mu[j] <- ( beta[1] + u[subj[j],1] ) +
    ( beta[2] + u[subj[j],2] ) * ( so[j] ) + w[item[j]]
    rrt[j] ~ dnorm( mu[j], tau.e )
}
    
    #------------------------------------------------------------
    # Priors:
    
    # Fixed intercept and slope (uninformative)
    beta[1] ~ dnorm(0.0,1.0E-5)
    beta[2] ~ dnorm(0.0,1.0E-5)
    
    # Residual variance
    tau.e <- pow(sigma.e,-2)
    sigma.e  ~ dunif(0,100)
    
    # Define prior for the variance-covariance matrix of the random effects for subjects
    ## precision:
    Omega.u  ~ dwish( R, 2 )
    ## R matrix:
    R[1,1] <- pow(sigma.a,2)
    R[2,2] <- pow(sigma.b,2)
    R[1,2] <- rho*sigma.a*sigma.b
    R[2,1] <- R[1,2]
    ## Vcov matrix:
    Sigma.u <- inverse(Omega.u)
    ## priors for var int. var slopes
    sigma.a ~ dunif(0,10)
    sigma.b ~ dunif(0,10)
    
    ## prior for correlation:
    #rho ~ dunif(-1,1)
    rho <- rho2*2-1
    rho2 ~ dbeta(1.5,1.5)

    # Between-item variation
    tau.w <- pow(sigma.w,-2)
    sigma.w  ~ dunif(0,10)   
}",
     file="JAGSmodels/gwintslopeuninfcorr.jag" )
@


<<echo=F>>=
#headnoun<-subset(data,region=="headnoun")

headnoun.dat <- list(subj=
                       sort(as.integer( 
                         factor(headnoun$subj) )),
                     item=
                       sort(as.integer( 
                         factor(headnoun$item) )),
                     rrt = headnoun$rrt,
                     so = headnoun$so,
                     N = nrow(headnoun),
                     I = 
                       length( unique(headnoun$subj) ),
                     K = 
                       length( unique(headnoun$item) )  
)

library(rjags)
 
track.variables<-c("beta","sigma.e","sigma.w","rho","sigma.a","sigma.b")

#                   "smallest","mn","largest")

headnoun.mod <- jags.model( file = "JAGSmodels/gwintslopeuninfcorr.jag",
                         data = headnoun.dat,
                     n.chains = 4,
                      n.adapt =2000 , quiet=T)


headnoun.res <- coda.samples( headnoun.mod,
                              var = track.variables,
                              n.iter = 10000,
                              thin = 20 )

#summary(headnoun.res)
#plot(headnoun.res)

mcmcChain<-as.matrix(headnoun.res)
@

<<fig=T>>=
hist(mcmcChain[,3])
@


\section{Fitting LMMs in Stan}

As soon as we transition to more complex models, we will need Stan; JAGS has its limits.
I will cover three basic types of models that we will fit in Stan:

\begin{enumerate}
\item 
Varying intercepts for subject and item
\item
Varying intercepts and varying slopes for subject, and varying intercepts for items.
\end{enumerate}

Recall that in textbooks and in these lecture notes, we refer to the normal distribution in terms of the variance: $N(\mu,\sigma^2)$. In R, we refer to it in terms of standard deviation: $N(\mu,\sigma)$. Then came WinBUGS and JAGS, which refers to the normal in terms of precision:
$N(\mu,1/\sigma^2)$. In Stan, the normal distribution is like R, referred to in terms of the standard deviation. Make sure you keep that in mind.

First make sure you have RStan installed. See the RStan website for details.
I begin by fitting a model with varying intercept by subject and by item.
I leave this uncommented because it's pretty obvious what the code is doing, given that we have seen similar JAGS code before.

<<echo=T,eval=F>>=
library(rstan)
set_cppo("fast")  # for best running speed

#set_cppo('debug') # make debug easier

## Gibson and Wu in RStan
## varying intercepts for subjects and items:

data<-read.table("data/gibsonwu2012data.txt",header=T)
## convert to reciprocal rt:
data$rrt<- -1000/data$rt
## contrast coding:
data$so <- ifelse(
  data$type%in%c("subj-ext"),-0.5,0.5)

## reference values from lmer:
library(lme4)
(m1 <- lmer(rrt~so+(1|subj),
            subset(data,region=="headnoun")))

headnoun<-subset(data,region=="headnoun")

## Stan ready data:
headnoun.dat <- list(subj=
                    sort(as.integer( 
                    factor(headnoun$subj) )),
                    item=sort(as.integer( 
                         factor(headnoun$item) )),
                     rrt = headnoun$rrt,
                     so = headnoun$so,
                     N = nrow(headnoun),
                     I = 
                       length( unique(headnoun$subj) ),
                     K = 
                       length( unique(headnoun$item) )  
)

## Stan code:
varying.int_code<-'
  data {
int<lower=1> N; //no. of rows
real so[N];     // predictor
real rrt[N];   //outcome
int<lower=1> I;   //number of subjects
int<lower=1, upper=I> subj[N];  //subject id
int<lower=1> K;   //number of items
int<lower=1, upper=K> item[N];  //item id
}
parameters {
real alpha;      // intercept
real beta;       // slope
real u[I];   // random intercept subj
real w[K];   // random intercept item
real<lower=0> sigma_e;  // residual variance 
real<lower=0> sigma_u;   //  subject var
real<lower=0> sigma_w;   //  item var
}
model {
real mu[N];
for (i in 1:N) {
mu[i] <- alpha + beta*so[i] + u[subj[i]]+w[item[i]];
}
rrt ~ normal(mu,sigma_e);    // likelihood
alpha ~ normal(0,5); 
beta ~ normal(0,5); 
u ~ normal(0,sigma_u);
w ~ normal(0,sigma_w);
## could have used uniform:
sigma_u ~ cauchy(0,5);
sigma_w ~ cauchy(0,5);
sigma_e ~ cauchy(0,5);
}
'

fit <- stan(model_code = varying.int_code, 
            data = headnoun.dat, 
            iter = 500, chains = 2)

#print(fit)
#plot(fit)

## matches up:
m1<-lmer(rrt~so+(1|subj)+(1|item),headnoun)
@

Next we look at the model with varying intercepts and varying slopes by subject. Here, we need to know how obtain the variance-covariance matrix ($\Sigma$) from the correlation matrix which, 
just to confuse the issue, we will call $\Omega$ (not to be confused with $\Omega$, the precision matrix we used earlier to fit the varying intercept and varying slope model in JAGS). 

In order to define a prior for the variance-covariance matrix, we will proceed as follows. We will first define a prior for the correlation matrix $\Omega$ using the Stan function:

\begin{verbatim}
Omega ~ lkj_corr(2.0)	
\end{verbatim} 

This function generates a random correlation matrix whose distribution 
depends on a single ``eta'' parameter. 
This eta parameter can be treated as the shape parameter of a symmetric beta distribution. 
If eta = 1 then its distribution 
is jointly uniform; if it's greater than 1, then the lkj\_corr distribution is concentrated around the identity matrix, which has 1's in the diagonals and 0's in the off-diagonals (recall that it's a multivariate distribution), and as eta increases, the distribution becomes more sharply concentrated around the identity matrix. If eta lies between 0 and 1, then there is a trough at the identity matrix.

Let's say we have defined a prior for the correlation matrix, and a prior for each variance component in the variance-covariance matrix (e.g., one for the standard deviation of the varying intercepts, and one for the standard deviation of the varying slopes).  
We can now derive the prior distribution of the variance-covariance matrix. Suppose that we have a 2x2 correlation matrix, Omega, and two standard deviations, let's say these have value 1 and 2. We can simply multiply the correlation matrix with the standard deviations to get the variance covariance matrix. 
%To do this we create a diagonal matrix D which has the standard deviations in the diagonals, and 0's elsewhere.

<<echo=F>>=
## 2x2 correlation matrix:
Omega <- matrix(c(1.00, 0.25,
                  0.25, 1.00),ncol=2)
 
## sds of the two variance components as a diagonal matrix:
sigmas <- c(1,2)
D <- diag(sigmas)

## Recover Sigma:
Sigma <- t(D)*Omega*D 
@

%You can do the same multiplication using a for-loop:

<<>>=
## 2x2 correlation matrix:
Omega <- matrix(c(1.00, 0.25,
                  0.25, 1.00),ncol=2)

sigmas <- c(1,2)

## create empty 2x2 matrix:
Sigma <- matrix(rep(NA,4),ncol=2)

## 
for (r in 1:2) {
for (c in 1:2) {
print(sigmas[r] * sigmas[c] * Omega[r,c])	
Sigma[r,c] <- sigmas[r] * sigmas[c] * Omega[r,c]
}
}
@

This is how we are going to specify the prior for the variance-covariance matrix. Recall that in the JAGS model we used the Wishart distribution; we could have done that here too (exercise).

<<>>=
## varying intercepts and varying slopes model:
## of Gibson and Wu data:

headnoun.dat <- list(zero=c(0,0),
                     subj=
                       sort(as.integer( 
                         factor(headnoun$subj) )),
                     item=sort(as.integer( 
                       factor(headnoun$item) )),
                     rrt = headnoun$rrt,
                     so = headnoun$so,
                     N = nrow(headnoun),
                     I = 
                       length( unique(headnoun$subj) ),
                     K = 
                       length( unique(headnoun$item) )  
)


varyingintslopes_code<- 'data {
int<lower=1> N;
real rrt[N];   //outcome
real so[N];   //predictor
int<lower=1> I;   //number of subjects
int<lower=1> K;   //number of items
int<lower=1, upper=I> subj[N];  //subject id
int<lower=1, upper=K> item[N];  //item id
vector[2] zero; //vector of zeros passed in from R
}
parameters {
vector[2] beta;      // intercept and slope
vector[2] u[I];   // random intercept and slope
real w[K];   // random intercept item
real<lower = 0> sigma_e;  // residual sd 
vector<lower=0>[2] sigma_u;   // subj sd
real<lower=0> sigma_w;   // item sd
// Note: Omega is the correlation matrix, not the precision matrix.
// We are going to build Sigma from Omega.
corr_matrix[2] Omega;     // correlation matrix for random intercepts and slopes
}
transformed parameters {
cov_matrix[2] Sigma;  // constructing the variance/cov matrix for random effects
for (r in 1:2) {
for (c in 1:2) {
Sigma[r,c] <- sigma_u[r] * sigma_u[c] * Omega[r,c];
}
}
}
model {
real mu[N];   // mu for likelihood
for (i in 1:I) u[i] ~ multi_normal(zero, Sigma);  // loop for subj random effects
for (k in 1:K) w[k] ~ normal(0,sigma_w); 
// loop for item random effects
for (n in 1:N) {
mu[n] <- beta[1] + beta[2]*so[n] + u[subj[n], 1] + u[subj[n], 2]*so[n];
}
rrt ~ normal(mu,sigma_e);    // likelihood
beta ~ normal(0,5);
sigma_e ~ cauchy(0,2);
sigma_u ~ cauchy(0,2);
sigma_w ~ cauchy(0,2);
Omega ~ lkj_corr(2.0);
}
'

library(rstan)
set_cppo("fast")  # for best running speed

fit <- stan(model_code = varyingintslopes_code, 
            data = headnoun.dat, 
            iter = 500, chains = 2)
 
#print(fit)
@

\section{Fitting models without correlation estimated for varying intercepts vs varying slopes}

Earlier I had discussed how to fit models with varying intercepts and slopes, but without a correlation estimated. This corresponds to models with a specification like:

\begin{verbatim}
... (1|subj)+(so-1|subj)...
\end{verbatim}

%Andrew Gelman mentions (p.c., and Stan mailing list) that in lmer the covariance is estimated using MLE, which leads to a noisy point estimate.
%%to-do need to understand this
%So, his recommendation is to always estimate a correlation parameter.
%However, in Stan it is fine to fit models without a correlation parameter because here we are ``averaging over the posterior distribution of the covariance matrix rather than using a noisy point estimate''.

Here are two ways of fitting such a model. Compare this to the JAGS models fit earlier (page \pageref{nocorrelationexamples}).

<<echo=T,eval=F>>=
## varying intercepts and varying slopes

headnoun.dat <- list(zero=c(0,0),
                     subj=
                       sort(as.integer( 
                         factor(headnoun$subj) )),
                     item=sort(as.integer( 
                       factor(headnoun$item) )),
                     rrt = headnoun$rrt,
                     so = headnoun$so,
                     N = nrow(headnoun),
                     I = 
                       length( unique(headnoun$subj) ),
                     K = 
                       length( unique(headnoun$item) )  
)


varyingintslopes_code<- 'data {
int<lower=1> N;
real rrt[N];   //outcome
real so[N];   //predictor
int<lower=1> I;   //number of subjects
int<lower=1> K;   //number of items
int<lower=1, upper=I> subj[N];  //subject id
int<lower=1, upper=K> item[N];  //item id
vector[2] zero; //vector of zeros passed in from R
}
parameters {
vector[2] beta;      // intercept and slope
vector[2] u[I];   // random intercept and slope
real w[K];   // random intercept item
real<lower = 0> sigma_e;  // residual sd 
vector<lower=0>[2] sigma_u;   // subj sd
real<lower=0> sigma_w;   // item sd
// Note: Omega is the correlation matrix, not the precision matrix.
// We are going to build Sigma from Omega.
corr_matrix[2] Omega;     // correlation matrix for random intercepts and slopes
}
transformed parameters {
cov_matrix[2] Sigma;  // constructing the variance/cov matrix for random effects
for (r in 1:2) {
for (c in 1:2) {
Sigma[r,c] <- sigma_u[r] * sigma_u[c] * Omega[r,c];
}
}
}
model {
real mu[N];   // mu for likelihood
for (i in 1:I) u[i] ~ multi_normal(zero, Sigma);  // loop for subj random effects
for (k in 1:K) w[k] ~ normal(0,sigma_w); 
// loop for item random effects
for (n in 1:N) {
mu[n] <- beta[1] + beta[2]*so[n] + u[subj[n], 1] + u[subj[n], 2]*so[n];
}
rrt ~ normal(mu,sigma_e);    // likelihood
beta ~ normal(0,10);
sigma_e ~ normal(0,10);
sigma_u ~ normal(0,10);
sigma_w ~ normal(0,10);
Omega ~ lkj_corr(2.0);
}
'

library(rstan)
set_cppo("fast")  # for best running speed

fit <- stan(model_code = varyingintslopes_code, 
            data = headnoun.dat, 
            iter = 500, chains = 2)

##print(fit)
##              Stan  lmer
#subjintercept   0.6  0.610
#subjslope       0.1  0.229
#item            6.3  0.331 <= don't match
#error           1.0  0.946
#m2<-lmer(rrt~so+(1+so|subj)+(1|item),headnoun)
#VarCorr(m2)

## second version:
headnoun.dat <- list(subj=
                       sort(as.integer( 
                         factor(headnoun$subj) )),
                     item=sort(as.integer( 
                       factor(headnoun$item) )),
                     rrt = headnoun$rrt,
                     so = headnoun$so,
                     N = nrow(headnoun),
                     I = 
                       length( unique(headnoun$subj) ),
                     K = 
                       length( unique(headnoun$item) )  
)


varyingintslopes_codev2 <- 'data {
int<lower=1> N;
real rrt[N];   //outcome
real so[N];   //predictor
int<lower=1> I;   //number of subjects
int<lower=1> K;   //number of items
int<lower=1, upper=I> subj[N];  //subject id
int<lower=1, upper=K> item[N];  //item id
}
parameters {
vector[2] beta;      // intercept and slope
vector[2] u[I];   // random intercept and slope
real w[K];   // random intercept item
real<lower=0> sigma_e;  // residual sd 
vector<lower=0>[2] sigma_u;   // subj sd
real<lower=0> sigma_w;   // item sd
}
model {
real mu[N];   // mu for likelihood
for (i in 1:I) u[i,1] ~ normal(0, sigma_u[1]);  
for (i in 1:I) u[i,2] ~ normal(0, sigma_u[2]);  
for (k in 1:K) w[k] ~ normal(0,sigma_w); 
for (n in 1:N) {
mu[n] <- beta[1] + beta[2]*so[n] + u[subj[n], 1] + u[subj[n], 2]*so[n];
}
rrt ~ normal(mu,sigma_e);    // likelihood
beta ~ normal(0,5);
sigma_e ~ normal(0,5);
sigma_u ~ normal(0,5);
sigma_w ~ normal(0,5);
}
'

library(rstan)
set_cppo("fast")  # for best running speed

fitv2 <- stan(model_code = varyingintslopes_codev2, 
            data = headnoun.dat, 
            iter = 1000, chains = 2)

##print(fitv2)
##              Stan            lmer
#subjintercept   0.6            0.610
#subjslope       0.2 (was 0.1)  0.229
#item            4.2 (was 6.3)  0.331 <= don't match
#error           1.0            0.946
m3<-lmer(rrt~so+(1|subj)+(so-1|subj)+(1|item),headnoun)
VarCorr(m3)
@

\section{Fitting even more complex linear mixed models}

If our random effects exceeds 2, i.e., if we have more than two variance components for random effects, we will probably be better off doing the modeling in Stan because defining priors for var-cov matrices of dimension greater than 2x2 is easier in Stan.

\subsection{Example: Kliegl et al 2011}

I benefitted greatly from discussion with members of the Stan mailing list for this example. In particular, thanks go to Sergio Polini for showing me how to write efficient code for such problems.

This is an example of a fairly involved dataset which can't easily be fit in JAGS. We also show here how parallelization of chains can be carried out when multiple cores are available on your machine. 

One thing to note here is that we use the lkj\_corr function in Stan. This was discussed earlier in the notes on priors for variance-covariance matrices (see page~\pageref{lkjcorrreference}).

The Stan code (save this as a text file called "kliegl2.stan" and put it in the same directory that you call the R code from below):

\begin{verbatim}
data {
    int<lower=1> N;
    real rt[N];                     //outcome
    real c1[N];                     //predictor
    real c2[N];                     //predictor
    real c3[N];                     //predictor
    int<lower=1> I;                 //number of subjects
    int<lower=1, upper=I> id[N];    //subject id
    vector[4] mu_prior;             //vector of zeros passed in from R
}
transformed data {
    real ZERO;                      // like #define ZERO 0 in C/C++
    ZERO <- 0.0;
}
parameters {
    vector[4] beta;                 // intercept and slope
    vector[4] u[I];                 // random intercept and slopes
    real<lower=0> sigma_e;          // residual sd
    vector<lower=0>[4] sigma_u;     // subj sd
    corr_matrix[4] Omega;           // correlation matrix for random intercepts and slopes
}
transformed parameters {
    matrix[4,4] D;
    D <- diag_matrix(sigma_u);
}
model {
    matrix[4,4] L;
    matrix[4,4] DL;
    real mu[N]; // mu for likelihood
    //priors:
    beta ~ normal(0,50);
    sigma_e ~ normal(0,100);
    sigma_u ~ normal(0,100);
    Omega ~ lkj_corr(4.0);
    
    L <- cholesky_decompose(Omega);
    for (m in 1:4)
        for (n in 1:m)
            DL[m,n] <- L[m,n] * sigma_u[m];
    for (m in 1:4)
        for (n in (m+1):4)
            DL[m,n] <- ZERO;
    
    for (i in 1:I)                  // loop for subj random effects
        u[i] ~ multi_normal_cholesky(mu_prior, DL);
    
    for (n in 1:N) {
        mu[n] <- beta[1] + beta[2]*c1[n] + beta[3]*c2[n] + beta[4]*c3[n] 
            + u[id[n], 1] + u[id[n], 2]*c1[n] + u[id[n], 3]*c2[n] + u[id[n], 4]*c3[n];
    }
    rt ~ normal(mu,sigma_e);        // likelihood
}
generated quantities {
    cov_matrix[4] Sigma;
    Sigma <- D * Omega * D;
}
\end{verbatim}

And here is the model fit (not run, because it takes quite long).

<<echo=T,eval=F>>=
library(rstan)
library(parallel)

load("KWDYZ_test.rda")

dat2 <- list(mu_prior=c(0,0,0,0),
                     id=sort(as.integer(factor(dat$id))),
                     rt = dat$rt,
                     c1 = dat$c1,
                     c2 = dat$c2,
                     c3 = dat$c3,
                     N = nrow(dat),
                     I = length(unique(dat$id)))  

kliegl2.sm <- stan_model("kliegl2.stan", model_name = "kliegl")
sflist <- mclapply(1:4, mc.cores = detectCores(),
                   function(i) sampling(kliegl2.sm, data = dat2,
                       chains = 1, chain_id = i, seed = 12345))
kliegl2.sf <- sflist2stanfit(sflist)

print(kliegl2.sf)
@

The results look like this:

\begin{verbatim}
Stan:
4 chains, each with iter=2000; warmup=1000; thin=1; 
post-warmup draws per chain=1000, total post-warmup draws=4000.
                 mean se_mean 
beta[1]        382.1     0.8
beta[2]         32.1     0.2
beta[3]         14.1     0.1
beta[4]          2.9     0.1
sigma_u[1]      56.8     0.1
sigma_u[2]      23.6     0.1
sigma_u[3]       9.3     0.4
sigma_u[4]       9.5     0.2
sigma_e         69.8     0.0

lmer:
          Estimate Std. Error t value
beta[1]   389.73   7.15    54.5
beta[2]   33.78    3.31    10.2
beta[3]   13.98    2.32     6.0
beta[4]   2.75     2.22     1.2
                         Var     sd
# id       (Intercept) 3098.3   55.66                         
#          c1           550.8   23.47     0.603               
#          c2           121.0   11.00    -0.129 -0.014        
#          c3            93.2    9.65    -0.247 -0.846  0.376
# Residual             4877.1   69.84
\end{verbatim}

These results are fairly similar to the lmer model's.

Here is another version of the Stan code, due to Sergio Polini:

\begin{verbatim}
data {
    int<lower=1> N;
    real rt[N];                     //outcome
    real c1[N];                     //predictor
    real c2[N];                     //predictor
    real c3[N];                     //predictor
    int<lower=1> I;                 //number of subjects
    int<lower=1, upper=I> id[N];    //subject id
    vector[4] mu_prior;             //vector of zeros passed in from R
}
transformed data {
    real ZERO;                      // like #define ZERO 0 in C/C++
    ZERO <- 0.0;
}
parameters {
    vector[4] beta;                 // intercept and slope
    vector[4] u[I];                 // random intercept and slopes
    real<lower=0> sigma_e;          // residual sd
    vector<lower=0>[4] sigma_u;     // subj sd
    corr_matrix[4] Omega;           // correlation matrix for random intercepts and slopes
}
transformed parameters {
    matrix[4,4] D;
    D <- diag_matrix(sigma_u);
}
model {
    matrix[4,4] L;
    matrix[4,4] DL;
    real mu[N]; // mu for likelihood
    //priors:
    beta ~ normal(0,50);
    sigma_e ~ normal(0,100);
    sigma_u ~ normal(0,100);
    Omega ~ lkj_corr(4.0);
    
    L <- cholesky_decompose(Omega);
    for (m in 1:4)
        for (n in 1:m)
            DL[m,n] <- L[m,n] * sigma_u[m];
    for (m in 1:4)
        for (n in (m+1):4)
            DL[m,n] <- ZERO;
    
    for (i in 1:I)                  // loop for subj random effects
        u[i] ~ multi_normal_cholesky(mu_prior, DL);
    
    for (n in 1:N) {
        mu[n] <- beta[1] + beta[2]*c1[n] + beta[3]*c2[n] + beta[4]*c3[n] 
            + u[id[n], 1] + u[id[n], 2]*c1[n] + u[id[n], 3]*c2[n] + u[id[n], 4]*c3[n];
    }
    rt ~ normal(mu,sigma_e);        // likelihood
}
generated quantities {
    cov_matrix[4] Sigma;
    for (i in 1:4)                  // diagonal and lower triangle
        for (j in 1:i)
            Sigma[i,j] <- Omega[i,j] * sigma_u[i] * sigma_u[j];
    for (i in 1:3)                  // upper triangle
        for (j in (i+1):4)
            Sigma[i,j] <- Sigma[j,i];
}
\end{verbatim}

\section{Developing a workflow for Stan modeling}

Sergio Polini uses the following conventions for modeling. This seems very sensible because every data analysis will have a very predictable format. Currently this is an issue because even leading statisticians like Andrew Gelman report (various blog entries) that they are unable to keep their files in an orderly enough manner to release them to the public domain.
I myself have a great deal of trouble trying to maintain and release code related to published research.

Given the serious (non-)replicability issues we face in psycholinguistics, it would be helpful if we release all data and code on publication. The workflow shown below might make this job easier. 

\begin{verbatim}
Sergio's method for maintaining data and code:
- "foo": model_name
- "foo.stan": model source
- "foo.data.R": data file
- "foo.sm": compiled model
- "foo.sm.RData": saved compiled model
- "foo.sf": final stanfit object
- "foo.sf.RData": eventual saved stanfit object

and a function:

mystan <- function(model, data=model, iter = 2000, seed = 12345, new=FALSE, ...) {
    stopifnot(require(parallel)) # should never happen!
    model.data <- paste(data, ".data.R", sep='')
    model.file <- paste(model, ".stan", sep='')
    model.sm <- paste(model, ".sm", sep='')
    model.saved <- paste(model.sm, ".RData", sep='')
    model.sf <- paste(model, ".sf", sep='')
    if (!file.exists(model.file))
        stop(paste(model.stan, "not found"))
    if (!file.exists(model.data))
        stop(paste(model.data, "not found"))
    if (new) { # you have a previous compiled model, but you've changed the source
        if (exists(model.sm))
            rm(list=model.sm, envir = .GlobalEnv)
        if (file.exists(model.saved))
            unlink(model.saved)
    }
    if (!exists(model.sm)) {
        if (file.exists(model.saved)) {
            # load the saved model and put its name in .GlobalEnv
            load(model.saved, envir = .GlobalEnv, verbose = FALSE)
        } else {
            rt <- stanc(model.file, model_name = model)
            sm <- stan_model(stanc_ret = rt)
            # create the compiled model variable name
            assign(model.sm, sm, envir = .GlobalEnv)
            save(list=model.sm, file = model.saved)
        }
    }
    sm <- get(model.sm) # sm <- the model to whom its variable name refers
    sflist <- mclapply(1:4, mc.cores = detectCores(),
                       function(i) sampling(sm, data = read_rdump(model.data),
                           chains = 1, chain_id = i,
                           iter = iter, seed = seed, ...))
    sf <- sflist2stanfit(sflist)
    # create the stanfit object variable name
    assign(model.sf, sf, .GlobalEnv)
}

An example session:

> library(rstan)
...
> source("mystan.R")
> N <- 100
> x <- rnorm(N,0,4)
> y <- 2 + 0.5 * x - 1.5 * x^2 + rnorm(N)
> stan_rdump(c("N", "x", "y"), file = "linreg.data.R")
> ls()
[1] "mystan" "N"      "x"      "y"     
> mystan("linreg1", "linreg")
...
> ls()
[1] "linreg1.sf" "linreg1.sm" "mystan"     "N"          "x"         
[6] "y"         
> print(linreg1.sf)
...
> mystan("linreg2", "linreg")
...
> ls()
[1] "linreg1.sf" "linreg1.sm" "linreg2.sf" "linreg2.sm" "mystan"    
[6] "N"          "x"          "y"         
> print(linreg2.sf)
...
\end{verbatim}
